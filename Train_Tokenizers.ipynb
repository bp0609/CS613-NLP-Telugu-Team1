{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-15T08:57:29.792345Z",
     "iopub.status.busy": "2024-11-15T08:57:29.791886Z",
     "iopub.status.idle": "2024-11-15T08:57:31.099349Z",
     "shell.execute_reply": "2024-11-15T08:57:31.097779Z",
     "shell.execute_reply.started": "2024-11-15T08:57:29.792301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import re\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:57:34.720203Z",
     "iopub.status.busy": "2024-11-15T08:57:34.719594Z",
     "iopub.status.idle": "2024-11-15T08:57:39.213819Z",
     "shell.execute_reply": "2024-11-15T08:57:39.212597Z",
     "shell.execute_reply.started": "2024-11-15T08:57:34.720160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo = \"ai4bharat/sangraha\"\n",
    "sb_folder = \"verified/tel\"\n",
    "local_dir = \"ai4bharat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:57:41.070122Z",
     "iopub.status.busy": "2024-11-15T08:57:41.069492Z",
     "iopub.status.idle": "2024-11-15T08:57:41.078215Z",
     "shell.execute_reply": "2024-11-15T08:57:41.076790Z",
     "shell.execute_reply.started": "2024-11-15T08:57:41.070077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(repo, sb_folder, local_dir, range_):\n",
    "    telugu_char_pattern = re.compile(r'[^\\u0C00-\\u0C7F\\u0000-\\u007F\\s<>\\n0-9.,!?]')\n",
    "    filenames = [f\"data-{i}.parquet\" for i in range(*range_)]\n",
    "    df_dataset = pd.DataFrame()\n",
    "    for filename in filenames:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo,\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=sb_folder,\n",
    "            filename=filename,\n",
    "            local_dir=local_dir\n",
    "        )\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df = df.rename(columns={\"text\": \"Input\"})\n",
    "        df.drop(columns=['type'], inplace=True)\n",
    "        df.drop(columns=['doc_id'], inplace=True)\n",
    "        df[\"Input\"] = \"<bos> \" + df[\"Input\"] + \" <eos>\"\n",
    "        df[\"Input\"] = df[\"Input\"].str.replace(\"\\n\", \"<newline>\")\n",
    "        df[\"Input\"] = df[\"Input\"].apply(lambda x: telugu_char_pattern.sub(\"\", x))\n",
    "        df_dataset = pd.concat([df_dataset, df], ignore_index=True)\n",
    "    \n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:57:42.980995Z",
     "iopub.status.busy": "2024-11-15T08:57:42.980546Z",
     "iopub.status.idle": "2024-11-15T08:58:00.023115Z",
     "shell.execute_reply": "2024-11-15T08:58:00.022019Z",
     "shell.execute_reply.started": "2024-11-15T08:57:42.980955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = get_dataset(repo, sb_folder, local_dir, (13, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:00.025422Z",
     "iopub.status.busy": "2024-11-15T08:58:00.025014Z",
     "iopub.status.idle": "2024-11-15T08:58:00.047093Z",
     "shell.execute_reply": "2024-11-15T08:58:00.045867Z",
     "shell.execute_reply.started": "2024-11-15T08:58:00.025382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bos&gt; వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;bos&gt; ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?&lt;newline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bos&gt; సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bos&gt; మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!&lt;newl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bos&gt; ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174757</th>\n",
       "      <td>&lt;bos&gt; ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174758</th>\n",
       "      <td>&lt;bos&gt; భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>&lt;bos&gt; నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174760</th>\n",
       "      <td>&lt;bos&gt; తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174761</th>\n",
       "      <td>&lt;bos&gt; 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174762 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Input\n",
       "0       <bos> వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....\n",
       "1       <bos> ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?<newline...\n",
       "2       <bos> సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...\n",
       "3       <bos> మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!<newl...\n",
       "4       <bos> ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...\n",
       "...                                                   ...\n",
       "174757  <bos> ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...\n",
       "174758  <bos> భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...\n",
       "174759  <bos> నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...\n",
       "174760  <bos> తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...\n",
       "174761  <bos> 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...\n",
       "\n",
       "[174762 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\x1f', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', 'ఁ', 'ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఌ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'ఙ', 'చ', 'ఛ', 'జ', 'ఝ', 'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'ఴ', 'వ', 'శ', 'ష', 'స', 'హ', 'ఽ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ౄ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్', 'ౕ', 'ౖ', 'ౘ', 'ౙ', 'ౠ', 'ౡ', 'ౢ', '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯', '౹', '౼', '౽']\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for i in range(len(df)):\n",
    "    unique_chars.update(set(df.iloc[i]['Input']))\n",
    "\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:00.049085Z",
     "iopub.status.busy": "2024-11-15T08:58:00.048585Z",
     "iopub.status.idle": "2024-11-15T08:58:00.206220Z",
     "shell.execute_reply": "2024-11-15T08:58:00.204819Z",
     "shell.execute_reply.started": "2024-11-15T08:58:00.049008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713.6504907608032"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_size(df):\n",
    "    return sys.getsizeof(df) / 1024 / 1024\n",
    "\n",
    "get_size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:29.661815Z",
     "iopub.status.busy": "2024-11-15T08:58:29.661377Z",
     "iopub.status.idle": "2024-11-15T08:58:30.519056Z",
     "shell.execute_reply": "2024-11-15T08:58:30.517904Z",
     "shell.execute_reply.started": "2024-11-15T08:58:29.661774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36732.6868535516 73465.3737071032 122442.28951183865\n",
      "150.839937210083 301.4163007736206 500.55063247680664\n"
     ]
    }
   ],
   "source": [
    "rows = df.shape[0]\n",
    "\n",
    "rows_150 = 150/get_size(df) * rows\n",
    "rows_300 = 300/get_size(df) * rows\n",
    "rows_500 = 500/get_size(df) * rows\n",
    "print(rows_150, rows_300, rows_500)\n",
    "\n",
    "df150 = df.sample(int(rows_150))\n",
    "df300 = df.sample(int(rows_300))\n",
    "df500 = df.sample(int(rows_500))\n",
    "print(get_size(df150), get_size(df300), get_size(df500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:31.408070Z",
     "iopub.status.busy": "2024-11-15T08:58:31.406943Z",
     "iopub.status.idle": "2024-11-15T08:58:31.502046Z",
     "shell.execute_reply": "2024-11-15T08:58:31.500914Z",
     "shell.execute_reply.started": "2024-11-15T08:58:31.407993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_150 = df150['Input'].tolist()\n",
    "text_300 = df300['Input'].tolist()\n",
    "text_500 = df500['Input'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:33.714360Z",
     "iopub.status.busy": "2024-11-15T08:58:33.713346Z",
     "iopub.status.idle": "2024-11-15T08:58:33.766318Z",
     "shell.execute_reply": "2024-11-15T08:58:33.765304Z",
     "shell.execute_reply.started": "2024-11-15T08:58:33.714313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, ByteLevelBPETokenizer, Tokenizer, CharBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def train_tokenizer(data_list, tokenizer_name, vocab_size = 32768, model_name = \"tokenizer\"):\n",
    "    if not os.path.exists(model_name):\n",
    "        os.makedirs(model_name)\n",
    "\n",
    "    bos_tok = \"<bos>\"\n",
    "    eos_tok = \"<eos>\"\n",
    "\n",
    "    special_char = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "    if (tokenizer_name == \"SentencePieceBPETokenizer\"):\n",
    "        tokenizer = SentencePieceBPETokenizer()\n",
    "        \n",
    "    if (tokenizer_name == \"ByteLevelBPETokenizer\"):\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "        \n",
    "    if (tokenizer_name == \"CharBPETokenizer\"):\n",
    "        tokenizer = CharBPETokenizer()\n",
    "        \n",
    "    if (tokenizer_name == \"WordPiece\"):\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
    "        trainer = WordPieceTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=[bos_tok, eos_tok, \"<unk>\", \"<pad>\", \"<newline>\"] + special_char,\n",
    "        )\n",
    "        tokenizer.train_from_iterator(data_list, trainer=trainer)\n",
    "        transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object=tokenizer,\n",
    "            bos_token = bos_tok,\n",
    "            eos_token = eos_tok,\n",
    "            unk_token = \"<unk>\",\n",
    "            pad_token = \"<pad>\",\n",
    "            mask_token = \"<mask>\",\n",
    "            padding_side = \"left\",\n",
    "            truncation_side = \"right\",\n",
    "            clean_up_tokenization_spaces = False,\n",
    "        )\n",
    "\n",
    "        transformer_tokenizer.save_pretrained(model_name)\n",
    "        print(f\"Tokenizer saved to {model_name}\")\n",
    "        return\n",
    "        \n",
    "    if (tokenizer_name == \"SentencePiece\"):\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=data_list,\n",
    "            model_prefix=model_name,\n",
    "            vocab_size=vocab_size,\n",
    "            bos_id=0,\n",
    "            eos_id=1,\n",
    "            pad_id=2,\n",
    "            unk_id=3,\n",
    "            character_coverage=1.0,\n",
    "            model_type=\"bpe\",\n",
    "            user_defined_symbols=special_char,\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    tokenizer.train_from_iterator(\n",
    "        data_list,\n",
    "        vocab_size = vocab_size,\n",
    "        min_frequency = 5,\n",
    "        special_tokens = [\"<pad>\", \"<unk>\", bos_tok, eos_tok, \"<newline>\"] + special_char,\n",
    "        show_progress = True,\n",
    "    )\n",
    "\n",
    "    transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token = bos_tok,\n",
    "        eos_token = eos_tok,\n",
    "        unk_token = \"<unk>\",\n",
    "        pad_token = \"<pad>\",\n",
    "        mask_token = \"<mask>\",\n",
    "        padding_side = \"left\",\n",
    "        truncation_side = \"right\",\n",
    "        clean_up_tokenization_spaces = False,\n",
    "    )\n",
    "\n",
    "    transformer_tokenizer.save_pretrained(model_name)\n",
    "    print(f\"Tokenizer saved to {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:38.099307Z",
     "iopub.status.busy": "2024-11-15T08:58:38.098775Z",
     "iopub.status.idle": "2024-11-15T08:58:39.422249Z",
     "shell.execute_reply": "2024-11-15T08:58:39.421057Z",
     "shell.execute_reply.started": "2024-11-15T08:58:38.099259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_fertility_score(texts,model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    total_tokens = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        total_tokens += len(tokens)\n",
    "        \n",
    "        words = text.split()\n",
    "        total_words += len(words)\n",
    "        \n",
    "    fertility_score = total_tokens / total_words\n",
    "    \n",
    "    print(f\"Fertility Score of {model_name}: {fertility_score:.2f}\")\n",
    "\n",
    "    return fertility_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T08:58:51.375849Z",
     "iopub.status.busy": "2024-11-15T08:58:51.375178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer saved to SentencePieceBPETokenizer_150\n",
      "Fertility Score of SentencePieceBPETokenizer_150: 1.68\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/v0v0mhyx1lq27mkj7vl7v5c80000gn/T/ipykernel_85815/1258564952.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer saved to SentencePieceBPETokenizer_300\n",
      "Fertility Score of SentencePieceBPETokenizer_300: 1.68\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer saved to SentencePieceBPETokenizer_500\n",
      "Fertility Score of SentencePieceBPETokenizer_500: 1.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import csv\n",
    "\n",
    "# tokenizer_names = [\"SentencePieceBPETokenizer\", \"ByteLevelBPETokenizer\", \"CharBPETokenizer\", \"WordPiece\", \"SentencePiece\"]\n",
    "tokenizer_names = [\"SentencePieceBPETokenizer\"]\n",
    "df = pd.DataFrame(columns=[\"Tokenizer\", \"Text\", \"Fertility Score\"])\n",
    "fertility_scores = dict()\n",
    "for tokenizer_name in tokenizer_names:\n",
    "    for text, text_name, df_subset in zip([text_150, text_300, text_500], [\"150\", \"300\", \"500\"], [df150, df300, df500]):\n",
    "        model_name = f\"{tokenizer_name}_{text_name}\"\n",
    "        train_tokenizer(text, tokenizer_name, model_name=model_name)\n",
    "        shutil.make_archive(model_name, 'zip', model_name)\n",
    "        fertility_scores[model_name] = get_fertility_score(text, model_name)\n",
    "        new_row = pd.DataFrame({\"Tokenizer\": [tokenizer_name], \"Text\": [text_name], \"Fertility Score\": [fertility_scores[model_name]]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv(f\"fertility_scores_{model_name}.csv\", index=False)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
