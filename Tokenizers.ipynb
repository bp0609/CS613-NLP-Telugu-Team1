{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T04:29:51.86555Z",
     "iopub.status.busy": "2024-11-13T04:29:51.865108Z",
     "iopub.status.idle": "2024-11-13T04:29:52.346991Z",
     "shell.execute_reply": "2024-11-13T04:29:52.346001Z",
     "shell.execute_reply.started": "2024-11-13T04:29:51.865497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:29:52.657497Z",
     "iopub.status.busy": "2024-11-13T04:29:52.656956Z",
     "iopub.status.idle": "2024-11-13T04:29:56.984416Z",
     "shell.execute_reply": "2024-11-13T04:29:56.983227Z",
     "shell.execute_reply.started": "2024-11-13T04:29:52.657454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo = \"ai4bharat/sangraha\"\n",
    "sb_folder = \"verified/tel\"\n",
    "local_dir = \"ai4bharat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:29:56.98675Z",
     "iopub.status.busy": "2024-11-13T04:29:56.986222Z",
     "iopub.status.idle": "2024-11-13T04:29:56.995634Z",
     "shell.execute_reply": "2024-11-13T04:29:56.994407Z",
     "shell.execute_reply.started": "2024-11-13T04:29:56.98671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataframe(repo, sb_folder, local_dir, i):\n",
    "    full_df = pd.DataFrame()\n",
    "\n",
    "    filename = f\"data-{i}.parquet\"\n",
    "    \n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=repo,\n",
    "        repo_type=\"dataset\",\n",
    "        subfolder=sb_folder,\n",
    "        filename=filename,\n",
    "        local_dir=local_dir\n",
    "    )\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df = df.rename(columns={\"text\": \"Input\"})\n",
    "    \n",
    "    full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:30:01.363347Z",
     "iopub.status.busy": "2024-11-13T04:30:01.362891Z",
     "iopub.status.idle": "2024-11-13T04:30:11.644533Z",
     "shell.execute_reply": "2024-11-13T04:30:11.643191Z",
     "shell.execute_reply.started": "2024-11-13T04:30:01.363301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = get_dataframe(repo,sb_folder,local_dir, 13)\n",
    "df.drop(columns=['type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2570c298eb9bfd667a35a84925e23b3f3f582f25</td>\n",
       "      <td>వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు. కానీ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256f3abe63fea4da5b7bd1d1854a3c28c207fac9</td>\n",
       "      <td>ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?\n",
       " LIC Policy: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2f69b38ac04110430d62f9a47e546e97c1e731dc</td>\n",
       "      <td>సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన నాణ్యత ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12bab9ea2b36680b67a4bb45e6e316cc98130895</td>\n",
       "      <td>మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!\n",
       " 13 ఏళ్ల త...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1c51460f1b5178a6238b73ed5830a15edaebd14a</td>\n",
       "      <td>ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించింది. వ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174757</th>\n",
       "      <td>8ad716a9bd687f55db28d6138a400c4a187b8ba4ec8dbe...</td>\n",
       "      <td>ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యనారాయణ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174758</th>\n",
       "      <td>5f9d6f029723dc1847ae8278df38627030c5e892</td>\n",
       "      <td>భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా పర్యటనక...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>4185f90a2f248c251a9ef416f468c88b7f6503ba</td>\n",
       "      <td>నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుంటూ సాగ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174760</th>\n",
       "      <td>3b7a66d7bb88a1526d26e21d9ee4a80c</td>\n",
       "      <td>తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్ కోహ్ల...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174761</th>\n",
       "      <td>9067282ae6b5ac69f933cb43c2423107407a3a61</td>\n",
       "      <td>1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం బేస్త...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174762 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   doc_id  \\\n",
       "0                2570c298eb9bfd667a35a84925e23b3f3f582f25   \n",
       "1                256f3abe63fea4da5b7bd1d1854a3c28c207fac9   \n",
       "2                2f69b38ac04110430d62f9a47e546e97c1e731dc   \n",
       "3                12bab9ea2b36680b67a4bb45e6e316cc98130895   \n",
       "4                1c51460f1b5178a6238b73ed5830a15edaebd14a   \n",
       "...                                                   ...   \n",
       "174757  8ad716a9bd687f55db28d6138a400c4a187b8ba4ec8dbe...   \n",
       "174758           5f9d6f029723dc1847ae8278df38627030c5e892   \n",
       "174759           4185f90a2f248c251a9ef416f468c88b7f6503ba   \n",
       "174760                   3b7a66d7bb88a1526d26e21d9ee4a80c   \n",
       "174761           9067282ae6b5ac69f933cb43c2423107407a3a61   \n",
       "\n",
       "                                                    Input  \n",
       "0       వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు. కానీ ...  \n",
       "1       ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?\n",
       " LIC Policy: ...  \n",
       "2       సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన నాణ్యత ...  \n",
       "3       మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!\n",
       " 13 ఏళ్ల త...  \n",
       "4       ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించింది. వ...  \n",
       "...                                                   ...  \n",
       "174757  ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యనారాయణ ...  \n",
       "174758  భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా పర్యటనక...  \n",
       "174759  నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుంటూ సాగ...  \n",
       "174760  తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్ కోహ్ల...  \n",
       "174761  1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం బేస్త...  \n",
       "\n",
       "[174762 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710.8633823394775"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_size(df):\n",
    "    return sys.getsizeof(df) / 1024 / 1024\n",
    "\n",
    "get_size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు. కానీ రియల్ లైఫ్ లో అలాంటి లవర్ బాయ్ ట్యాగ్ లైన్ సందీప్ కిషన్ కు ఇచ్చేయాల్సిందే. అవును. . ఇతడి ఎఫైర్లపై వచ్చినన్ని పుకార్లు అన్నీ ఇన్నీ కావు. మినిమం గ్యాప్స్ లో సందీప్ కిషన్ లవ్ ఎఫైర్లపై గాసిప్స్ వస్తూనే ఉంటాయి. వాటిలో కొన్నింటిని ఖండించిన సందీప్, మరికొన్నింటిపై మాత్రం రియాక్ట్ అవ్వలేదు. ఎట్టకేలకు తన లవ్ లైఫ్ పై స్పందించాడు సందీప్.\\n ఒకరు కాదు, ఇద్దరు కాదు. . ఏకంగా ముగ్గురితో డేటింగ్ చేశానని ఒప్పుకున్నాడు సందీప్ కిషన్. ఒక్కో ఏజ్ లో ఒక్కో అమ్మాయితో డేటింగ్ చేశానని, హీరోగా మారిన తర్వాతే ఈ ఎఫైర్లన్నీ నడిచాయని కూడా చెప్పుకొచ్చాడు. ఆ ముగ్గురు అమ్మాయిలు బ్రహ్మాండంగా ఉండేవారని, కానీ దురదృష్టవశాత్తూ వాళ్లలో ఏ ఒక్క రిలేషన్ షిప్ నిలబడలేదని ప్రకటించాడు. కానీ వాళ్ల ముగ్గుర్నుంచి తను చాలా కొత్త విషయాలు నేర్చుకున్నానని తెలిపాడు.\\n మూడు ఎఫైర్లు నడిపానని ప్రకటించిన సందీప్ కిషన్, ఆ అమ్మాయిల పేర్లు మాత్రం చెప్పలేదు. అంతేకాదు, భవిష్యత్తులో పెళ్లి చేసుకుంటే ప్రేమించి మాత్రమే పెళ్లాడతానని స్పష్టంచేశాడు. తన లేటెస్ట్ హీరోయిన్ అన్య సింగ్ తో తనకు ఎలాంటి ఎఫైర్ లేదంటున్న సందీప్ కిషన్. . దాదాపు రెండేళ్లుగా తను సింగిల్ గా ఉన్నట్టు ప్రకటించుకున్నాడు. '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Input\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2570c298eb9bfd667a35a84925e23b3f3f582f25</td>\n",
       "      <td>&lt;bos&gt; వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256f3abe63fea4da5b7bd1d1854a3c28c207fac9</td>\n",
       "      <td>&lt;bos&gt; ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?&lt;newline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2f69b38ac04110430d62f9a47e546e97c1e731dc</td>\n",
       "      <td>&lt;bos&gt; సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12bab9ea2b36680b67a4bb45e6e316cc98130895</td>\n",
       "      <td>&lt;bos&gt; మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!&lt;newl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1c51460f1b5178a6238b73ed5830a15edaebd14a</td>\n",
       "      <td>&lt;bos&gt; ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174757</th>\n",
       "      <td>8ad716a9bd687f55db28d6138a400c4a187b8ba4ec8dbe...</td>\n",
       "      <td>&lt;bos&gt; ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174758</th>\n",
       "      <td>5f9d6f029723dc1847ae8278df38627030c5e892</td>\n",
       "      <td>&lt;bos&gt; భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>4185f90a2f248c251a9ef416f468c88b7f6503ba</td>\n",
       "      <td>&lt;bos&gt; నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174760</th>\n",
       "      <td>3b7a66d7bb88a1526d26e21d9ee4a80c</td>\n",
       "      <td>&lt;bos&gt; తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174761</th>\n",
       "      <td>9067282ae6b5ac69f933cb43c2423107407a3a61</td>\n",
       "      <td>&lt;bos&gt; 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174762 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   doc_id  \\\n",
       "0                2570c298eb9bfd667a35a84925e23b3f3f582f25   \n",
       "1                256f3abe63fea4da5b7bd1d1854a3c28c207fac9   \n",
       "2                2f69b38ac04110430d62f9a47e546e97c1e731dc   \n",
       "3                12bab9ea2b36680b67a4bb45e6e316cc98130895   \n",
       "4                1c51460f1b5178a6238b73ed5830a15edaebd14a   \n",
       "...                                                   ...   \n",
       "174757  8ad716a9bd687f55db28d6138a400c4a187b8ba4ec8dbe...   \n",
       "174758           5f9d6f029723dc1847ae8278df38627030c5e892   \n",
       "174759           4185f90a2f248c251a9ef416f468c88b7f6503ba   \n",
       "174760                   3b7a66d7bb88a1526d26e21d9ee4a80c   \n",
       "174761           9067282ae6b5ac69f933cb43c2423107407a3a61   \n",
       "\n",
       "                                                    Input  \n",
       "0       <bos> వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....  \n",
       "1       <bos> ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?<newline...  \n",
       "2       <bos> సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...  \n",
       "3       <bos> మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!<newl...  \n",
       "4       <bos> ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...  \n",
       "...                                                   ...  \n",
       "174757  <bos> ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...  \n",
       "174758  <bos> భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...  \n",
       "174759  <bos> నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...  \n",
       "174760  <bos> తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...  \n",
       "174761  <bos> 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...  \n",
       "\n",
       "[174762 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Input\"] = \"<bos> \" + df[\"Input\"] + \" <eos>\"\n",
    "df[\"Input\"] = df[\"Input\"].str.replace(\"\\n\", \"<newline>\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు. కానీ రియల్ లైఫ్ లో అలాంటి లవర్ బాయ్ ట్యాగ్ లైన్ సందీప్ కిషన్ కు ఇచ్చేయాల్సిందే. అవును. . ఇతడి ఎఫైర్లపై వచ్చినన్ని పుకార్లు అన్నీ ఇన్నీ కావు. మినిమం గ్యాప్స్ లో సందీప్ కిషన్ లవ్ ఎఫైర్లపై గాసిప్స్ వస్తూనే ఉంటాయి. వాటిలో కొన్నింటిని ఖండించిన సందీప్, మరికొన్నింటిపై మాత్రం రియాక్ట్ అవ్వలేదు. ఎట్టకేలకు తన లవ్ లైఫ్ పై స్పందించాడు సందీప్.<newline> ఒకరు కాదు, ఇద్దరు కాదు. . ఏకంగా ముగ్గురితో డేటింగ్ చేశానని ఒప్పుకున్నాడు సందీప్ కిషన్. ఒక్కో ఏజ్ లో ఒక్కో అమ్మాయితో డేటింగ్ చేశానని, హీరోగా మారిన తర్వాతే ఈ ఎఫైర్లన్నీ నడిచాయని కూడా చెప్పుకొచ్చాడు. ఆ ముగ్గురు అమ్మాయిలు బ్రహ్మాండంగా ఉండేవారని, కానీ దురదృష్టవశాత్తూ వాళ్లలో ఏ ఒక్క రిలేషన్ షిప్ నిలబడలేదని ప్రకటించాడు. కానీ వాళ్ల ముగ్గుర్నుంచి తను చాలా కొత్త విషయాలు నేర్చుకున్నానని తెలిపాడు.<newline> మూడు ఎఫైర్లు నడిపానని ప్రకటించిన సందీప్ కిషన్, ఆ అమ్మాయిల పేర్లు మాత్రం చెప్పలేదు. అంతేకాదు, భవిష్యత్తులో పెళ్లి చేసుకుంటే ప్రేమించి మాత్రమే పెళ్లాడతానని స్పష్టంచేశాడు. తన లేటెస్ట్ హీరోయిన్ అన్య సింగ్ తో తనకు ఎలాంటి ఎఫైర్ లేదంటున్న సందీప్ కిషన్. . దాదాపు రెండేళ్లుగా తను సింగిల్ గా ఉన్నట్టు ప్రకటించుకున్నాడు.  <eos>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Input\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:30:11.67485Z",
     "iopub.status.busy": "2024-11-13T04:30:11.674524Z",
     "iopub.status.idle": "2024-11-13T04:30:11.680905Z",
     "shell.execute_reply": "2024-11-13T04:30:11.679786Z",
     "shell.execute_reply.started": "2024-11-13T04:30:11.674801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rows = df.shape[0]\n",
    "\n",
    "rows_150 = 150/get_size(df) * rows\n",
    "rows_300 = 300/get_size(df) * rows\n",
    "rows_500 = 500/get_size(df) * rows\n",
    "print(rows_150, rows_300, rows_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df150 = df.sample(int(rows_150))\n",
    "df300 = df.sample(int(rows_300))\n",
    "df500 = df.sample(int(rows_500))\n",
    "print(get_size(df150), get_size(df300), get_size(df500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:30:11.689412Z",
     "iopub.status.busy": "2024-11-13T04:30:11.68903Z",
     "iopub.status.idle": "2024-11-13T04:30:11.714809Z",
     "shell.execute_reply": "2024-11-13T04:30:11.713514Z",
     "shell.execute_reply.started": "2024-11-13T04:30:11.689372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_150 = df150['Input'].tolist()\n",
    "text_300 = df300['Input'].tolist()\n",
    "text_500 = df500['Input'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T16:22:56.502692Z",
     "iopub.status.busy": "2024-11-12T16:22:56.502262Z",
     "iopub.status.idle": "2024-11-12T16:22:57.210904Z",
     "shell.execute_reply": "2024-11-12T16:22:57.209798Z",
     "shell.execute_reply.started": "2024-11-12T16:22:56.502653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, ByteLevelBPETokenizer, Tokenizer, CharBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def train_tokenizer(data_list, tokenizer_name, vocab_size = 32768, model_name = \"tokenizer\"):\n",
    "    if not os.path.exists(model_name):\n",
    "        os.makedirs(model_name)\n",
    "\n",
    "    bos_tok = \"<bos>\"\n",
    "    eos_tok = \"<eos>\"\n",
    "\n",
    "    special_char = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "    if (tokenizer_name == \"SentencePieceBPETokenizer\"):\n",
    "        tokenizer = SentencePieceBPETokenizer()\n",
    "        \n",
    "    if (tokenizer_name == \"ByteLevelBPETokenizer\"):\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "        \n",
    "    if (tokenizer_name == \"CharBPETokenizer\"):\n",
    "        tokenizer = CharBPETokenizer()\n",
    "        \n",
    "    if (tokenizer_name == \"WordPiece\"):\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
    "        trainer = WordPieceTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=[bos_tok, eos_tok, \"<unk>\", \"<pad>\"] + special_char,\n",
    "        )\n",
    "        tokenizer.train_from_iterator(data_list, trainer=trainer)\n",
    "        transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object=tokenizer,\n",
    "            bos_token = bos_tok,\n",
    "            eos_token = eos_tok,\n",
    "            unk_token = \"<unk>\",\n",
    "            pad_token = \"<pad>\",\n",
    "            mask_token = \"<mask>\",\n",
    "            padding_side = \"left\",\n",
    "            truncation_side = \"right\",\n",
    "            clean_up_tokenization_spaces = False,\n",
    "        )\n",
    "\n",
    "        transformer_tokenizer.save_pretrained(model_name)\n",
    "        print(f\"Tokenizer saved to {model_name}\")\n",
    "        return\n",
    "        \n",
    "    if (tokenizer_name == \"SentencePiece\"):\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=data_list,\n",
    "            model_prefix=model_name,\n",
    "            vocab_size=vocab_size,\n",
    "            bos_id=0,\n",
    "            eos_id=1,\n",
    "            pad_id=2,\n",
    "            unk_id=3,\n",
    "            character_coverage=1.0,\n",
    "            model_type=\"bpe\",\n",
    "            user_defined_symbols=special_char,\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    tokenizer.train_from_iterator(\n",
    "        data_list,\n",
    "        vocab_size = vocab_size,\n",
    "        min_frequency = 5,\n",
    "        special_tokens = [\"<pad>\", \"<unk>\", bos_tok, eos_tok] + special_char,\n",
    "        show_progress = True,\n",
    "    )\n",
    "\n",
    "    transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token = bos_tok,\n",
    "        eos_token = eos_tok,\n",
    "        unk_token = \"<unk>\",\n",
    "        pad_token = \"<pad>\",\n",
    "        mask_token = \"<mask>\",\n",
    "        padding_side = \"left\",\n",
    "        truncation_side = \"right\",\n",
    "        clean_up_tokenization_spaces = False,\n",
    "    )\n",
    "\n",
    "    transformer_tokenizer.save_pretrained(model_name)\n",
    "    print(f\"Tokenizer saved to {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_fertility_score(texts,model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    total_tokens = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        total_tokens += len(tokens)\n",
    "        \n",
    "        words = text.split()\n",
    "        total_words += len(words)\n",
    "    # print(total_words)\n",
    "    # print(total_tokens)\n",
    "    fertility_score = total_tokens / total_words\n",
    "    \n",
    "    # print(f\"Fertility Score of {model_name}: {fertility_score:.2f}\")\n",
    "\n",
    "    return fertility_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_names = [\"SentencePieceBPETokenizer\", \"ByteLevelBPETokenizer\", \"CharBPETokenizer\", \"WordPiece\", \"SentencePiece\"]\n",
    "df = pd.DataFrame()\n",
    "fertility_scores = dict()\n",
    "for tokenizer_name in tokenizer_names:\n",
    "    for text, text_name in zip([text_150, text_300, text_500], [\"150\", \"300\", \"500\"]):\n",
    "        model_name = f\"{tokenizer_name}_{text_name}\"\n",
    "        train_tokenizer(text, tokenizer_name, model_name=model_name)\n",
    "        fertility_scores[model_name] = get_fertility_score(text, model_name)\n",
    "        df = df.append({\"Tokenizer\": tokenizer_name, \"Text\": text_name, \"Fertility Score\": fertility_scores[model_name]}, ignore_index=True)\n",
    "        df.to_csv(\"fertility_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:31.927857Z",
     "iopub.status.busy": "2024-11-11T16:40:31.927375Z",
     "iopub.status.idle": "2024-11-11T16:40:31.936547Z",
     "shell.execute_reply": "2024-11-11T16:40:31.934774Z",
     "shell.execute_reply.started": "2024-11-11T16:40:31.927816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GemmaConfig, AutoTokenizer, AutoModel, MistralConfig, MistralModel, MistralForCausalLM, LlamaConfig, LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:32.497232Z",
     "iopub.status.busy": "2024-11-11T16:40:32.496645Z",
     "iopub.status.idle": "2024-11-11T16:40:32.533115Z",
     "shell.execute_reply": "2024-11-11T16:40:32.531187Z",
     "shell.execute_reply.started": "2024-11-11T16:40:32.497151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = LlamaConfig(hidden_size=512,\n",
    "                     vocab_size=len(tokenizer.vocab),\n",
    "                     num_attention_heads=4,\n",
    "                     num_key_value_heads=2,\n",
    "                     num_hidden_layers=12,\n",
    "                     intermediate_size=688,\n",
    "                     max_position_embeddings=128,\n",
    "                     bos_token_id=2,\n",
    "                     eos_token_id=3)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:33.314726Z",
     "iopub.status.busy": "2024-11-11T16:40:33.314256Z",
     "iopub.status.idle": "2024-11-11T16:40:34.26006Z",
     "shell.execute_reply": "2024-11-11T16:40:34.258693Z",
     "shell.execute_reply.started": "2024-11-11T16:40:33.314683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_mis = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:34.815781Z",
     "iopub.status.busy": "2024-11-11T16:40:34.81535Z",
     "iopub.status.idle": "2024-11-11T16:40:35.276883Z",
     "shell.execute_reply": "2024-11-11T16:40:35.275395Z",
     "shell.execute_reply.started": "2024-11-11T16:40:34.81574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i,j in model_mis.named_parameters():\n",
    "  if j.requires_grad and len(j.size()) > 1:\n",
    "    init.xavier_uniform_(j.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:36.073868Z",
     "iopub.status.busy": "2024-11-11T16:40:36.073401Z",
     "iopub.status.idle": "2024-11-11T16:40:36.081423Z",
     "shell.execute_reply": "2024-11-11T16:40:36.080269Z",
     "shell.execute_reply.started": "2024-11-11T16:40:36.073827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_param=0\n",
    "for i,j in model_mis.named_parameters():\n",
    "    total_param += j.numel()\n",
    "print(total_param/(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:37.453702Z",
     "iopub.status.busy": "2024-11-11T16:40:37.453243Z",
     "iopub.status.idle": "2024-11-11T16:40:37.915094Z",
     "shell.execute_reply": "2024-11-11T16:40:37.913856Z",
     "shell.execute_reply.started": "2024-11-11T16:40:37.453662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_mis.save_pretrained(\"test\")\n",
    "tokenizer.save_pretrained(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:38.195328Z",
     "iopub.status.busy": "2024-11-11T16:40:38.194119Z",
     "iopub.status.idle": "2024-11-11T16:40:38.202066Z",
     "shell.execute_reply": "2024-11-11T16:40:38.200351Z",
     "shell.execute_reply.started": "2024-11-11T16:40:38.195255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:40:39.418917Z",
     "iopub.status.busy": "2024-11-11T16:40:39.417357Z",
     "iopub.status.idle": "2024-11-11T16:40:39.424599Z",
     "shell.execute_reply": "2024-11-11T16:40:39.423305Z",
     "shell.execute_reply.started": "2024-11-11T16:40:39.418853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for i in input_ids:\n",
    "  token_list.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:41:27.648474Z",
     "iopub.status.busy": "2024-11-11T16:41:27.647967Z",
     "iopub.status.idle": "2024-11-11T16:41:27.656323Z",
     "shell.execute_reply": "2024-11-11T16:41:27.655131Z",
     "shell.execute_reply.started": "2024-11-11T16:41:27.648431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:41:28.413309Z",
     "iopub.status.busy": "2024-11-11T16:41:28.412872Z",
     "iopub.status.idle": "2024-11-11T16:41:28.421322Z",
     "shell.execute_reply": "2024-11-11T16:41:28.420064Z",
     "shell.execute_reply.started": "2024-11-11T16:41:28.413268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:41:31.106753Z",
     "iopub.status.busy": "2024-11-11T16:41:31.106319Z",
     "iopub.status.idle": "2024-11-11T16:41:31.117961Z",
     "shell.execute_reply": "2024-11-11T16:41:31.116747Z",
     "shell.execute_reply.started": "2024-11-11T16:41:31.106713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"input_ids\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:41:33.217109Z",
     "iopub.status.busy": "2024-11-11T16:41:33.216394Z",
     "iopub.status.idle": "2024-11-11T16:41:33.222743Z",
     "shell.execute_reply": "2024-11-11T16:41:33.221536Z",
     "shell.execute_reply.started": "2024-11-11T16:41:33.217048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "context_len = 5\n",
    "token_batch = []\n",
    "for i in input_ids:\n",
    "  token_batch.append(token_list[:context_len])\n",
    "  token_list = token_list[context_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:42:25.027995Z",
     "iopub.status.busy": "2024-11-11T16:42:25.026629Z",
     "iopub.status.idle": "2024-11-11T16:42:25.035673Z",
     "shell.execute_reply": "2024-11-11T16:42:25.034477Z",
     "shell.execute_reply.started": "2024-11-11T16:42:25.027936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(token_batch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:42:26.428518Z",
     "iopub.status.busy": "2024-11-11T16:42:26.42805Z",
     "iopub.status.idle": "2024-11-11T16:42:26.44222Z",
     "shell.execute_reply": "2024-11-11T16:42:26.440974Z",
     "shell.execute_reply.started": "2024-11-11T16:42:26.428479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"input_ids\"] = token_batch\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:42:30.574008Z",
     "iopub.status.busy": "2024-11-11T16:42:30.573532Z",
     "iopub.status.idle": "2024-11-11T16:42:30.591907Z",
     "shell.execute_reply": "2024-11-11T16:42:30.590769Z",
     "shell.execute_reply.started": "2024-11-11T16:42:30.573967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df)\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:42:44.109995Z",
     "iopub.status.busy": "2024-11-11T16:42:44.109099Z",
     "iopub.status.idle": "2024-11-11T16:42:44.127203Z",
     "shell.execute_reply": "2024-11-11T16:42:44.126211Z",
     "shell.execute_reply.started": "2024-11-11T16:42:44.10995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "split_dataset = hf_dataset.train_test_split(test_size=0.1)  # Adjust test_size as needed\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:42:58.670947Z",
     "iopub.status.busy": "2024-11-11T16:42:58.670498Z",
     "iopub.status.idle": "2024-11-11T16:42:58.728068Z",
     "shell.execute_reply": "2024-11-11T16:42:58.726864Z",
     "shell.execute_reply.started": "2024-11-11T16:42:58.670905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset.to_parquet(\"test_dataset_token_test.parquet\")\n",
    "eval_dataset.to_parquet(\"test_dataset_token_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:43:08.902798Z",
     "iopub.status.busy": "2024-11-11T16:43:08.902372Z",
     "iopub.status.idle": "2024-11-11T16:43:25.132478Z",
     "shell.execute_reply": "2024-11-11T16:43:25.131027Z",
     "shell.execute_reply.started": "2024-11-11T16:43:08.902752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:43:25.135525Z",
     "iopub.status.busy": "2024-11-11T16:43:25.134719Z",
     "iopub.status.idle": "2024-11-11T16:44:02.028632Z",
     "shell.execute_reply": "2024-11-11T16:44:02.027228Z",
     "shell.execute_reply.started": "2024-11-11T16:43:25.135478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes trl peft -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:44:02.031475Z",
     "iopub.status.busy": "2024-11-11T16:44:02.031009Z",
     "iopub.status.idle": "2024-11-11T16:44:02.065747Z",
     "shell.execute_reply": "2024-11-11T16:44:02.064781Z",
     "shell.execute_reply.started": "2024-11-11T16:44:02.031432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:44:03.837752Z",
     "iopub.status.busy": "2024-11-11T16:44:03.837308Z",
     "iopub.status.idle": "2024-11-11T16:44:03.869721Z",
     "shell.execute_reply": "2024-11-11T16:44:03.868539Z",
     "shell.execute_reply.started": "2024-11-11T16:44:03.837709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_mis,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:45:37.765334Z",
     "iopub.status.busy": "2024-11-11T16:45:37.764843Z",
     "iopub.status.idle": "2024-11-11T16:45:37.770664Z",
     "shell.execute_reply": "2024-11-11T16:45:37.769412Z",
     "shell.execute_reply.started": "2024-11-11T16:45:37.765291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:45:45.983282Z",
     "iopub.status.busy": "2024-11-11T16:45:45.982809Z",
     "iopub.status.idle": "2024-11-11T16:45:48.50188Z",
     "shell.execute_reply": "2024-11-11T16:45:48.500327Z",
     "shell.execute_reply.started": "2024-11-11T16:45:45.98324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T18:07:24.172203Z",
     "iopub.status.busy": "2024-11-11T18:07:24.171727Z",
     "iopub.status.idle": "2024-11-11T18:07:26.025984Z",
     "shell.execute_reply": "2024-11-11T18:07:26.024158Z",
     "shell.execute_reply.started": "2024-11-11T18:07:24.172158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, LlamaConfig, LlamaForCausalLM, TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "def load_and_prepare_tokenizer(model_path):\n",
    "    \"\"\"Loads the tokenizer and prepares tokenized inputs.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token as eos token for consistency\n",
    "    return tokenizer\n",
    "\n",
    "def add_special_tokens(df, tokenizer):\n",
    "    \"\"\"Adds special tokens to each input sentence and tokenizes.\"\"\"\n",
    "    df['Input'] = '<bos>' + df['Input'] + '<eos>'\n",
    "    input_ids = tokenizer(df['Input'].to_list())[\"input_ids\"]\n",
    "    df[\"tokens\"] = input_ids\n",
    "    return df\n",
    "\n",
    "def configure_llama_model(tokenizer):\n",
    "    \"\"\"Sets up Llama model configuration and initializes the model.\"\"\"\n",
    "    config = LlamaConfig(\n",
    "        hidden_size=512,\n",
    "        vocab_size=len(tokenizer.vocab),\n",
    "        num_attention_heads=4,\n",
    "        num_key_value_heads=2,\n",
    "        num_hidden_layers=12,\n",
    "        intermediate_size=688,\n",
    "        max_position_embeddings=128,\n",
    "        bos_token_id=2,\n",
    "        eos_token_id=3\n",
    "    )\n",
    "    model = LlamaForCausalLM(config)\n",
    "    initialize_parameters(model)\n",
    "    return model\n",
    "\n",
    "def initialize_parameters(model):\n",
    "    \"\"\"Initializes model parameters with Xavier initialization.\"\"\"\n",
    "    for _, param in model.named_parameters():\n",
    "        if param.requires_grad and len(param.size()) > 1:\n",
    "            init.xavier_uniform_(param.data)\n",
    "\n",
    "def save_model_and_tokenizer(model, tokenizer, path=\"LLM_model\"):\n",
    "    \"\"\"Saves model and tokenizer to the specified directory.\"\"\"\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "def tokenize_and_batch_data(input_ids, context_len=128):\n",
    "    \"\"\"Splits tokenized data into smaller batches with a fixed context length.\"\"\"\n",
    "    token_list = [token for sublist in input_ids for token in sublist]  # Flatten the list\n",
    "    token_batches = [token_list[i:i+context_len] for i in range(0, len(token_list), context_len)]\n",
    "    return token_batches\n",
    "\n",
    "def create_datasets(df):\n",
    "    \"\"\"Converts DataFrame to HuggingFace Dataset and splits into train and eval sets.\"\"\"\n",
    "    hf_dataset = HFDataset.from_pandas(df)\n",
    "    split_dataset = hf_dataset.train_test_split(test_size=0.1)\n",
    "    return split_dataset['train'], split_dataset['test']\n",
    "\n",
    "def save_datasets(train_dataset, eval_dataset, path=\"test_dataset_token_test.parquet\"):\n",
    "    \"\"\"Saves the datasets in Parquet format.\"\"\"\n",
    "    train_dataset.to_parquet(path)\n",
    "    eval_dataset.to_parquet(path)\n",
    "\n",
    "def calculate_perplexity(trainer, epoch_intervals):\n",
    "    \"\"\"Calculates perplexity at specified epoch intervals and returns a matrix.\"\"\"\n",
    "    perplexity_matrix = {}\n",
    "    for epoch in epoch_intervals:\n",
    "        trainer.args.num_train_epochs = epoch\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        perplexity_matrix[f\"Epoch {epoch}\"] = torch.exp(torch.tensor(eval_results[\"eval_loss\"])).item()\n",
    "    return perplexity_matrix\n",
    "\n",
    "def generate_outputs(model, tokenizer, prompts):\n",
    "    \"\"\"Generates text outputs for a set of prompts using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        outputs.append(decoded_output)\n",
    "    return outputs\n",
    "\n",
    "def train_model(model, train_dataset, eval_dataset, tokenizer):\n",
    "    \"\"\"Trains the model and logs perplexity at specified intervals.\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=1,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    epoch_intervals = [0.1 * i for i in range(1, 11)]  # 0.1 to 1.0 epochs\n",
    "    perplexity_matrix = calculate_perplexity(trainer, epoch_intervals)\n",
    "    print(\"Perplexity Matrix:\", perplexity_matrix)\n",
    "    return trainer\n",
    "def get_dataframe(repo, sb_folder, local_dir, no_files=1):\n",
    "    full_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, no_files+1):\n",
    "        filename = f\"wiki_tel_Telu_000{i}_of_0063.parquet\"\n",
    "        \n",
    "        # Download the file from the Hugging Face hub\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo,\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=sb_folder,\n",
    "            filename=filename,\n",
    "            local_dir=local_dir\n",
    "        )\n",
    "    \n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        df = df.rename(columns={\"text\": \"Input\"})\n",
    "        \n",
    "        full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "\n",
    "    return full_df\n",
    "def main():\n",
    "    # Load tokenizer and data\n",
    "    repo = \"ai4bharat/sangraha\"\n",
    "    sb_folder = \"synthetic/tel_Telu\"\n",
    "    local_dir = \"ai4bharat\"\n",
    "    tokenizer = load_and_prepare_tokenizer(\"SentencePieceBPETokenizer_300\")\n",
    "    \n",
    "    df = get_dataframe(repo, sb_folder, local_dir)  # Load your data here\n",
    "    df = df[:]\n",
    "    df = add_special_tokens(df, tokenizer)\n",
    "    \n",
    "    # Configure and initialize the model\n",
    "    model = configure_llama_model(tokenizer)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    save_model_and_tokenizer(model, tokenizer)\n",
    "\n",
    "    # Tokenize and batch data\n",
    "    token_batches = tokenize_and_batch_data(df[\"tokens\"].to_list(), context_len=128)\n",
    "    df[\"input_ids\"] = token_batches\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset, eval_dataset = create_datasets(df)\n",
    "    save_datasets(train_dataset, eval_dataset)\n",
    "    \n",
    "    # Train model and get perplexity matrix\n",
    "    trainer = train_model(model, train_dataset, eval_dataset, tokenizer)\n",
    "\n",
    "    # Generate output for 10 prompts\n",
    "    prompts = [\"Example prompt 1\", \"Example prompt 2\", \"Example prompt 3\", \n",
    "               \"Example prompt 4\", \"Example prompt 5\", \"Example prompt 6\", \n",
    "               \"Example prompt 7\", \"Example prompt 8\", \"Example prompt 9\", \n",
    "               \"Example prompt 10\"]\n",
    "    outputs = generate_outputs(model, tokenizer, prompts)\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f\"Output for Prompt {i+1}: {output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '<|user|>\\n' + message['content'] + '<|end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|bot|>\\n' + message['content'] + '<|end|>\\n' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{{ eos_token }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"additional_special_tokens\": [\"<|user|>\", \"<|bot|>\", \"<|end|>\"] # same here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
