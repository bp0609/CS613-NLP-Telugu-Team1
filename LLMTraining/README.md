# CS613-NLP Team-1 Telugu

## Team Members

1. **Bhavik Patel (22110047)**
2. **Guntas Singh Saran (22110089)**
3. **Hitesh Kumar (22110098)**
4. **Ruchit Jagodara (22110102)**
5. **Jinil Patel (22110184)**



<h2> Task - 1 </h2>

Below is the graph between text length and fertility score of different models.

> **_NOTE:_** here, we have used text length as a measurement for dataset size.
> 
<div align = "center">
    <img src = "https://github.com/guntas-13/CS613-NLP/blob/main/LLMTraining/Tokenizers.png" style="width: 100%">
</div>


Code for training the tokenizers can be found in `CS613-NLP-Telugu-Team1/LLMTraining/Train_Tokenizers.ipynb`.

Here, final fertility scores of differnt models are as follows:-
1. ByteLevelBPETokenizer :- 5.8643
2. CharBPETokenizer :- 1.8153
3. SentencePieceBPETokenizer :- 1.6794
4. WordPiece :- 0.1637


<h2>Task - 2</h2>

Model Files (as seen in the sidebar)
https://iitgnacin-my.sharepoint.com/:f:/g/personal/22110089_iitgn_ac_in/EsPZ6u3kv-FKmkg7_eyV_pAB6TTc7vfVZv1HPKt6Abx-pA?e=kcfVjl

<div align = "center">
    <img src = "https://github.com/guntas-13/CS613-NLP/blob/main/LLMTraining/perplexity_vs_epoch.png" style="width: 100%">
</div>

