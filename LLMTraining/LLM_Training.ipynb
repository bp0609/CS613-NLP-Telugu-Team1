{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-15T19:37:21.314943Z",
     "iopub.status.busy": "2024-11-15T19:37:21.314626Z",
     "iopub.status.idle": "2024-11-15T19:37:22.410486Z",
     "shell.execute_reply": "2024-11-15T19:37:22.409376Z",
     "shell.execute_reply.started": "2024-11-15T19:37:21.314904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:37:31.932523Z",
     "iopub.status.busy": "2024-11-15T19:37:31.931746Z",
     "iopub.status.idle": "2024-11-15T19:37:52.570949Z",
     "shell.execute_reply": "2024-11-15T19:37:52.570185Z",
     "shell.execute_reply.started": "2024-11-15T19:37:31.932479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from transformers import (\n",
    "    AutoTokenizer, LlamaConfig, LlamaForCausalLM, TrainingArguments, Trainer, PreTrainedTokenizerFast, DataCollatorForLanguageModeling, TrainerCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo = \"ai4bharat/sangraha\"\n",
    "sb_folder = \"verified/tel\"\n",
    "local_dir = \"ai4bharat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:37:54.797875Z",
     "iopub.status.busy": "2024-11-15T19:37:54.796811Z",
     "iopub.status.idle": "2024-11-15T19:37:54.802117Z",
     "shell.execute_reply": "2024-11-15T19:37:54.801127Z",
     "shell.execute_reply.started": "2024-11-15T19:37:54.797831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_size(df):\n",
    "    return sys.getsizeof(df) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:37:56.162554Z",
     "iopub.status.busy": "2024-11-15T19:37:56.161797Z",
     "iopub.status.idle": "2024-11-15T19:37:56.172571Z",
     "shell.execute_reply": "2024-11-15T19:37:56.171467Z",
     "shell.execute_reply.started": "2024-11-15T19:37:56.162512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(repo, sb_folder, local_dir, range_):\n",
    "    telugu_char_pattern = re.compile(r'[^\\u0C00-\\u0C7F\\u0000-\\u007F\\s<>\\n0-9.,!?]')\n",
    "    filenames = [f\"data-{i}.parquet\" for i in range(*range_)]\n",
    "    df_dataset = pd.DataFrame()\n",
    "    for filename in filenames:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo,\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=sb_folder,\n",
    "            filename=filename,\n",
    "            local_dir=local_dir\n",
    "        )\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df = df.rename(columns={\"text\": \"Input\"})\n",
    "        df.drop(columns=['type'], inplace=True)\n",
    "        df.drop(columns=['doc_id'], inplace=True)\n",
    "        df[\"Input\"] = \"<bos> \" + df[\"Input\"] + \" <eos>\"\n",
    "        df[\"Input\"] = df[\"Input\"].str.replace(\"\\n\", \"<newline>\")\n",
    "        df[\"Input\"] = df[\"Input\"].apply(lambda x: telugu_char_pattern.sub(\"\", x))\n",
    "        df_dataset = pd.concat([df_dataset, df], ignore_index=True)\n",
    "    \n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:38:00.921855Z",
     "iopub.status.busy": "2024-11-15T19:38:00.920678Z",
     "iopub.status.idle": "2024-11-15T19:38:25.635648Z",
     "shell.execute_reply": "2024-11-15T19:38:25.634700Z",
     "shell.execute_reply.started": "2024-11-15T19:38:00.921792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bos&gt; వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;bos&gt; ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?&lt;newline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bos&gt; సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bos&gt; మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!&lt;newl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bos&gt; ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174757</th>\n",
       "      <td>&lt;bos&gt; ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174758</th>\n",
       "      <td>&lt;bos&gt; భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>&lt;bos&gt; నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174760</th>\n",
       "      <td>&lt;bos&gt; తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174761</th>\n",
       "      <td>&lt;bos&gt; 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174762 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Input\n",
       "0       <bos> వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....\n",
       "1       <bos> ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?<newline...\n",
       "2       <bos> సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...\n",
       "3       <bos> మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!<newl...\n",
       "4       <bos> ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...\n",
       "...                                                   ...\n",
       "174757  <bos> ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...\n",
       "174758  <bos> భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...\n",
       "174759  <bos> నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...\n",
       "174760  <bos> తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...\n",
       "174761  <bos> 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...\n",
       "\n",
       "[174762 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_dataset(repo, sb_folder, local_dir, (13, 14))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:38:25.638515Z",
     "iopub.status.busy": "2024-11-15T19:38:25.637764Z",
     "iopub.status.idle": "2024-11-15T19:38:25.771351Z",
     "shell.execute_reply": "2024-11-15T19:38:25.770427Z",
     "shell.execute_reply.started": "2024-11-15T19:38:25.638462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Size: 710.9850740432739 MB\n",
      "New Data Size: 406.81468296051025 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Data Size: {get_size(df)} MB\")\n",
    "df = df[:100000]\n",
    "print(f\"New Data Size: {get_size(df)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:38:25.772795Z",
     "iopub.status.busy": "2024-11-15T19:38:25.772406Z",
     "iopub.status.idle": "2024-11-15T19:38:25.925922Z",
     "shell.execute_reply": "2024-11-15T19:38:25.925152Z",
     "shell.execute_reply.started": "2024-11-15T19:38:25.772750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/input/sentencepiecebpe/SentencePieceBPETokenizer_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:38:25.929044Z",
     "iopub.status.busy": "2024-11-15T19:38:25.928357Z",
     "iopub.status.idle": "2024-11-15T19:39:48.628178Z",
     "shell.execute_reply": "2024-11-15T19:39:48.627347Z",
     "shell.execute_reply.started": "2024-11-15T19:38:25.928996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def tokenize_function(df, context_length=256):\n",
    "    \n",
    "    input_ids = tokenizer(df[\"Input\"].tolist())[\"input_ids\"]\n",
    "    flattened_ids = list(chain.from_iterable(input_ids))\n",
    "    \n",
    "    token_chunks = [\n",
    "        flattened_ids[i:i + context_length]\n",
    "        for i in range(0, len(flattened_ids), context_length)\n",
    "    ]\n",
    "    \n",
    "    last_chunk = token_chunks[-1]\n",
    "    last_chunk_length = len(last_chunk)\n",
    "    padding_length = context_length - last_chunk_length\n",
    "    last_chunk += [tokenizer.pad_token_id] * padding_length\n",
    "    token_chunks[-1] = last_chunk\n",
    "\n",
    "    return pd.DataFrame({\"input_ids\": token_chunks})\n",
    "\n",
    "tokenized_dataset = tokenize_function(df, context_length=256)\n",
    "tokenized_dataset = Dataset.from_pandas(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:39:48.631045Z",
     "iopub.status.busy": "2024-11-15T19:39:48.630748Z",
     "iopub.status.idle": "2024-11-15T19:39:48.636047Z",
     "shell.execute_reply": "2024-11-15T19:39:48.635102Z",
     "shell.execute_reply.started": "2024-11-15T19:39:48.631013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 167421\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:39:54.637238Z",
     "iopub.status.busy": "2024-11-15T19:39:54.636837Z",
     "iopub.status.idle": "2024-11-15T19:41:22.301736Z",
     "shell.execute_reply": "2024-11-15T19:41:22.300607Z",
     "shell.execute_reply.started": "2024-11-15T19:39:54.637199Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6958, 1877, 722, 12949, 2506, 8536, 1343, 314, 3761, 3411, 2659, 5653, 362, 4539, 1899, 259, 5743, 5279, 18936, 3204, 1302, 18867, 2549, 1229, 405, 16773, 1723, 4, 266, 5161, 15085, 10317, 2669, 224, 722, 13447, 11576, 9355, 17425, 722, 1291, 5002, 5108, 2851, 27346, 7039, 8304, 104, 4, 438, 29041, 21523, 5784, 7213, 17944, 2864, 7007, 1909, 29161, 21313, 33, 4, 438, 17918, 11159, 261, 2303, 231, 163, 14627, 33, 4, 438, 6497, 283, 348, 143, 3655, 303, 33, 4, 438, 234, 1171, 25089, 234, 1171, 1252, 407, 1245, 22192, 882, 1189, 22159, 23960, 201, 33, 4, 5862, 5857, 23133, 17470, 8304, 407, 581, 10377, 2793, 5099, 6955, 4204, 15584, 32033, 24073, 1020, 4553, 5071, 2351, 6599, 21881, 3377, 386, 13576, 7764, 10317, 2669, 224, 722, 692, 14292, 393, 4039, 7493, 28, 11455, 4813, 522, 597, 256, 12887, 18, 2659, 5653, 6875, 16792, 3879, 7184, 5555, 14256, 407, 14707, 23508, 13308, 1021, 6875, 512, 3856, 995, 1284, 537, 4235, 1607, 1172, 8066, 294, 3230, 10831, 16469, 7977, 1872, 20839, 5559, 5489, 3920, 711, 6875, 512, 4794, 2380, 671, 7972, 5279, 3057, 2512, 1802, 3734, 355, 1079, 587, 3230, 3856, 1402, 4, 20604, 6471, 880, 18292, 6471, 880, 14719, 3182, 362, 18330, 3416, 9071, 5223, 355, 9418, 3104, 414, 3106, 225, 7491, 345, 3340, 4542, 21263, 379, 1954, 4024, 5739, 9355, 10377, 208, 15567, 11521, 11786, 684, 314, 4247, 2098, 4678, 189, 7, 5, 1323, 77, 13726, 2943, 1042, 22566, 5653, 18867, 829, 1091, 3384, 212, 2549, 1229, 405, 14591, 7362, 10377, 208, 2786, 21491, 829]\n",
      "\n",
      "[2617, 811, 2052, 1257, 17388, 469, 952, 9699, 362, 4669, 1784, 154, 1778, 279, 4731, 272, 7626, 1138, 28, 2122, 7401, 1708, 225, 2235, 158, 4, 189, 3, 2, 487, 3305, 385, 11256, 9829, 1621, 5284, 6691, 1573, 332, 937, 13138, 1158, 385, 30719, 11668, 332, 4625, 734, 2534, 23300, 622, 937, 225, 2021, 394, 19310, 24397, 592, 1358, 395, 528, 6117, 1258, 332, 1496, 2430, 12846, 1571, 21125, 3834, 13138, 3918, 31188, 1984, 1632, 788, 2021, 394, 2430, 12846, 398, 21125, 3141, 2914, 10461, 30, 4, 1203, 114, 103, 670, 1415, 686, 2989, 394, 2595, 415, 411, 504, 299, 1282, 20547, 2525, 761, 3146, 2963, 394, 3940, 760, 332, 3918, 770, 2706, 27504, 21073, 1121, 1255, 2032, 1911, 23398, 8721, 365, 1632, 9157, 103, 1942, 686, 788, 23300, 20316, 1925, 266, 1942, 385, 11766, 1822, 1604, 10590, 20180, 134, 27504, 2963, 1121, 3568, 13656, 332, 5576, 5895, 2550, 6387, 23300, 3936, 686, 1255, 257, 24397, 6595, 2534, 23300, 5088, 1215, 8154, 5088, 297, 10461, 30, 359, 19310, 24397, 592, 4442, 13179, 3040, 3967, 12359, 332, 359, 6594, 2728, 411, 504, 299, 394, 1284, 3283, 2532, 5123, 7348, 189, 189, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"input_ids\"][4])\n",
    "print()\n",
    "print(tokenized_dataset[\"input_ids\"][-1])\n",
    "print()\n",
    "print(len(tokenized_dataset[\"input_ids\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:41:22.305330Z",
     "iopub.status.busy": "2024-11-15T19:41:22.304506Z",
     "iopub.status.idle": "2024-11-15T19:41:23.101456Z",
     "shell.execute_reply": "2024-11-15T19:41:23.100559Z",
     "shell.execute_reply.started": "2024-11-15T19:41:22.305294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llama_config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,                       # d_model = 512\n",
    "    num_hidden_layers=6,                   # L = 6\n",
    "    num_attention_heads=8,                 # n_h = 8\n",
    "    intermediate_size=768,\n",
    "    max_position_embeddings=128,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config=llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:41:54.509829Z",
     "iopub.status.busy": "2024-11-15T19:41:54.508831Z",
     "iopub.status.idle": "2024-11-15T19:41:54.811542Z",
     "shell.execute_reply": "2024-11-15T19:41:54.810560Z",
     "shell.execute_reply.started": "2024-11-15T19:41:54.509785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 46.930432M\n"
     ]
    }
   ],
   "source": [
    "for i,j in model.named_parameters():\n",
    "  if j.requires_grad and len(j.size()) > 1:\n",
    "    init.xavier_uniform_(j.data)\n",
    "    \n",
    "total_param=0\n",
    "for i,j in model.named_parameters():\n",
    "    total_param += j.numel()\n",
    "print(f\"Total Number of Parameters: {total_param/(10**6)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:42:00.558935Z",
     "iopub.status.busy": "2024-11-15T19:42:00.558542Z",
     "iopub.status.idle": "2024-11-16T04:21:52.749809Z",
     "shell.execute_reply": "2024-11-16T04:21:52.749092Z",
     "shell.execute_reply.started": "2024-11-15T19:42:00.558895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cc7548b1f2444c88c624b858d0f955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112923033334079, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241115_194213-jldvs8uf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface/runs/jldvs8uf' target=\"_blank\">llama_telugu</a></strong> to <a href='https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface/runs/jldvs8uf' target=\"_blank\">https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface/runs/jldvs8uf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94180' max='94180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94180/94180 8:39:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.244300</td>\n",
       "      <td>8.245357</td>\n",
       "      <td>3809.893671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.776300</td>\n",
       "      <td>7.710895</td>\n",
       "      <td>2232.539624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.347900</td>\n",
       "      <td>7.343413</td>\n",
       "      <td>1545.979356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.069000</td>\n",
       "      <td>7.059713</td>\n",
       "      <td>1164.110886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.833800</td>\n",
       "      <td>6.827794</td>\n",
       "      <td>923.151720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.694300</td>\n",
       "      <td>6.639542</td>\n",
       "      <td>764.744374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.508900</td>\n",
       "      <td>6.477914</td>\n",
       "      <td>650.612569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>6.334700</td>\n",
       "      <td>6.335208</td>\n",
       "      <td>564.086955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>6.232700</td>\n",
       "      <td>6.212095</td>\n",
       "      <td>498.744921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>6.089600</td>\n",
       "      <td>6.110009</td>\n",
       "      <td>450.342641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.925700</td>\n",
       "      <td>6.017190</td>\n",
       "      <td>410.423675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.889400</td>\n",
       "      <td>5.936759</td>\n",
       "      <td>378.705552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.858400</td>\n",
       "      <td>5.864536</td>\n",
       "      <td>352.318575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.742400</td>\n",
       "      <td>5.800482</td>\n",
       "      <td>330.458893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.713300</td>\n",
       "      <td>5.741276</td>\n",
       "      <td>311.461665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.707700</td>\n",
       "      <td>5.689550</td>\n",
       "      <td>295.760476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>5.588400</td>\n",
       "      <td>5.643314</td>\n",
       "      <td>282.397001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.629300</td>\n",
       "      <td>5.598078</td>\n",
       "      <td>269.907086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>5.465300</td>\n",
       "      <td>5.560774</td>\n",
       "      <td>260.023978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>5.391000</td>\n",
       "      <td>5.530308</td>\n",
       "      <td>252.221646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>5.380000</td>\n",
       "      <td>5.500209</td>\n",
       "      <td>244.743159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>5.441600</td>\n",
       "      <td>5.472286</td>\n",
       "      <td>238.003588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>5.362400</td>\n",
       "      <td>5.443717</td>\n",
       "      <td>231.300221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>5.348100</td>\n",
       "      <td>5.419197</td>\n",
       "      <td>225.697833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>5.394100</td>\n",
       "      <td>5.395084</td>\n",
       "      <td>220.320632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>5.260400</td>\n",
       "      <td>5.371717</td>\n",
       "      <td>215.232099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>5.256700</td>\n",
       "      <td>5.349674</td>\n",
       "      <td>210.539598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>5.293300</td>\n",
       "      <td>5.331354</td>\n",
       "      <td>206.717710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>5.184500</td>\n",
       "      <td>5.318851</td>\n",
       "      <td>204.149179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>5.132300</td>\n",
       "      <td>5.304210</td>\n",
       "      <td>201.181947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>5.204300</td>\n",
       "      <td>5.288963</td>\n",
       "      <td>198.137913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>5.131400</td>\n",
       "      <td>5.274340</td>\n",
       "      <td>195.261591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>5.099800</td>\n",
       "      <td>5.258799</td>\n",
       "      <td>192.250383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>5.116500</td>\n",
       "      <td>5.246450</td>\n",
       "      <td>189.890948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>5.135500</td>\n",
       "      <td>5.232036</td>\n",
       "      <td>187.173522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>5.122000</td>\n",
       "      <td>5.219886</td>\n",
       "      <td>184.913071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>5.082200</td>\n",
       "      <td>5.207005</td>\n",
       "      <td>182.546429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>5.004200</td>\n",
       "      <td>5.199982</td>\n",
       "      <td>181.269009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>5.032700</td>\n",
       "      <td>5.193894</td>\n",
       "      <td>180.168836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>4.965600</td>\n",
       "      <td>5.185852</td>\n",
       "      <td>178.725668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>5.041800</td>\n",
       "      <td>5.177620</td>\n",
       "      <td>177.260502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>4.941400</td>\n",
       "      <td>5.168126</td>\n",
       "      <td>175.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>4.943500</td>\n",
       "      <td>5.159277</td>\n",
       "      <td>174.038657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>4.978300</td>\n",
       "      <td>5.151054</td>\n",
       "      <td>172.613395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>4.950900</td>\n",
       "      <td>5.142009</td>\n",
       "      <td>171.059044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>4.931600</td>\n",
       "      <td>5.132435</td>\n",
       "      <td>169.429231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>4.929800</td>\n",
       "      <td>5.124117</td>\n",
       "      <td>168.025692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>4.881500</td>\n",
       "      <td>5.128127</td>\n",
       "      <td>168.700781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>4.848900</td>\n",
       "      <td>5.124316</td>\n",
       "      <td>168.059186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>4.817200</td>\n",
       "      <td>5.118966</td>\n",
       "      <td>167.162451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>4.887500</td>\n",
       "      <td>5.113194</td>\n",
       "      <td>166.200429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>4.868400</td>\n",
       "      <td>5.106828</td>\n",
       "      <td>165.145716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>4.857600</td>\n",
       "      <td>5.099960</td>\n",
       "      <td>164.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>4.854500</td>\n",
       "      <td>5.094333</td>\n",
       "      <td>163.094974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>4.861900</td>\n",
       "      <td>5.087499</td>\n",
       "      <td>161.984255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>4.845700</td>\n",
       "      <td>5.080396</td>\n",
       "      <td>160.837763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>4.754100</td>\n",
       "      <td>5.085192</td>\n",
       "      <td>161.610998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>4.762900</td>\n",
       "      <td>5.084850</td>\n",
       "      <td>161.555677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>4.774400</td>\n",
       "      <td>5.081648</td>\n",
       "      <td>161.039210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>4.796200</td>\n",
       "      <td>5.078455</td>\n",
       "      <td>160.525924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>4.755700</td>\n",
       "      <td>5.074148</td>\n",
       "      <td>159.835906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>4.803000</td>\n",
       "      <td>5.070008</td>\n",
       "      <td>159.175645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>4.798400</td>\n",
       "      <td>5.064888</td>\n",
       "      <td>158.362705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>4.750200</td>\n",
       "      <td>5.061120</td>\n",
       "      <td>157.767047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>4.767300</td>\n",
       "      <td>5.056556</td>\n",
       "      <td>157.048669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>4.673900</td>\n",
       "      <td>5.054854</td>\n",
       "      <td>156.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>4.645700</td>\n",
       "      <td>5.060484</td>\n",
       "      <td>157.666798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>4.753500</td>\n",
       "      <td>5.059971</td>\n",
       "      <td>157.585999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>4.672200</td>\n",
       "      <td>5.057368</td>\n",
       "      <td>157.176327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>4.680900</td>\n",
       "      <td>5.054963</td>\n",
       "      <td>156.798746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>4.688500</td>\n",
       "      <td>5.051430</td>\n",
       "      <td>156.245771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>4.670500</td>\n",
       "      <td>5.048498</td>\n",
       "      <td>155.788319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>4.687900</td>\n",
       "      <td>5.046017</td>\n",
       "      <td>155.402215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>4.710800</td>\n",
       "      <td>5.042521</td>\n",
       "      <td>154.859851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>4.671000</td>\n",
       "      <td>5.038686</td>\n",
       "      <td>154.267144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>4.614900</td>\n",
       "      <td>5.045580</td>\n",
       "      <td>155.334353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>4.624500</td>\n",
       "      <td>5.045843</td>\n",
       "      <td>155.375245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>4.656500</td>\n",
       "      <td>5.045656</td>\n",
       "      <td>155.346131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>4.618100</td>\n",
       "      <td>5.043618</td>\n",
       "      <td>155.029857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>4.623100</td>\n",
       "      <td>5.041762</td>\n",
       "      <td>154.742412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>4.652400</td>\n",
       "      <td>5.039937</td>\n",
       "      <td>154.460287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>4.645500</td>\n",
       "      <td>5.038706</td>\n",
       "      <td>154.270307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>4.627600</td>\n",
       "      <td>5.036549</td>\n",
       "      <td>153.937798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>4.610200</td>\n",
       "      <td>5.034300</td>\n",
       "      <td>153.592091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>4.570500</td>\n",
       "      <td>5.035622</td>\n",
       "      <td>153.795242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>4.591400</td>\n",
       "      <td>5.038026</td>\n",
       "      <td>154.165443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>4.608500</td>\n",
       "      <td>5.038194</td>\n",
       "      <td>154.191322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>4.610800</td>\n",
       "      <td>5.037245</td>\n",
       "      <td>154.045005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>4.590700</td>\n",
       "      <td>5.036861</td>\n",
       "      <td>153.985885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>4.590500</td>\n",
       "      <td>5.036165</td>\n",
       "      <td>153.878794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>4.581600</td>\n",
       "      <td>5.035254</td>\n",
       "      <td>153.738638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>4.581500</td>\n",
       "      <td>5.034885</td>\n",
       "      <td>153.681981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>4.604900</td>\n",
       "      <td>5.033847</td>\n",
       "      <td>153.522530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>4.605900</td>\n",
       "      <td>5.033612</td>\n",
       "      <td>153.486444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1612.765 MB of 1790.635 MB uploaded\\r'), FloatProgress(value=0.9006662818029326, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▃▆█▃▂▄▂▄▃▃▆▅▄▂▄▄▄▄▅▃█▄▅▃▂▅▄▄▄▅▃▂▄▁▃▄▄▆▄</td></tr><tr><td>eval/samples_per_second</td><td>▃▅▃▅▆▅▇▅█▆▄▅▅▆▅▅▄▅▇▃▆▁▄▃▆▆▄▆▅▅▃▅▅▃▃▄▃▆▅▇</td></tr><tr><td>eval/steps_per_second</td><td>▄▃▅▆▆▅▃█▆▅▅▅▄▄▇▆▃▃▆▁▆▆▅▅▅▇▄▆▅▆▅▅▃▅▆▄▅▄▃▆</td></tr><tr><td>eval_perplexity</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▂▃▃▃▄▄▄▄▅▅▅▅▅▆▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇█▇▇█▇███</td></tr><tr><td>train/learning_rate</td><td>████████▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>5.03361</td></tr><tr><td>eval/runtime</td><td>108.0143</td></tr><tr><td>eval/samples_per_second</td><td>155.007</td></tr><tr><td>eval/steps_per_second</td><td>9.693</td></tr><tr><td>eval_perplexity</td><td>153.48644</td></tr><tr><td>total_flos</td><td>6.978702766768128e+16</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>94180</td></tr><tr><td>train/grad_norm</td><td>212738.26562</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>4.5498</td></tr><tr><td>train_loss</td><td>5.19325</td></tr><tr><td>train_runtime</td><td>31159.5977</td></tr><tr><td>train_samples_per_second</td><td>48.357</td></tr><tr><td>train_steps_per_second</td><td>3.023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama_telugu</strong> at: <a href='https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface/runs/jldvs8uf' target=\"_blank\">https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface/runs/jldvs8uf</a><br/> View project at: <a href='https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/guntas-13-indian-institute-of-technology-gandhinagar/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 30 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_194213-jldvs8uf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import wandb\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Data Collator for Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # For causal language modeling\n",
    ")\n",
    "\n",
    "wandb.init(project=\"huggingface\", name=\"llama_telugu\")\n",
    "\n",
    "# Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./llama_telugu_model\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,  # Number of full training epochs\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     save_steps=500,\n",
    "#     save_total_limit=2,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=100,  # Evaluate every 100 steps\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=50,\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     warmup_steps=500,\n",
    "#     fp16=True,  # Use mixed precision if possible\n",
    "# #     report_to=[\"none\"]  # Disable reporting for simplicity\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                # Set a local path to store results (checkpoints)\n",
    "    evaluation_strategy=\"steps\",           # Evaluate after every epoch\n",
    "    logging_dir=\"./logs\",                  # Directory for logs\n",
    "    logging_steps=50,                      # Log every 50 steps\n",
    "    save_steps=1000,                       # Save checkpoints every 1000 steps\n",
    "    eval_steps = 1000,\n",
    "    save_total_limit=3,                    # Limit the number of saved checkpoints\n",
    "    per_device_train_batch_size=8,         # Adjust batch size based on your memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,                    # Number of epochs\n",
    "    report_to=\"wandb\",                     # Log to W&B\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,           # Load the best model based on evaluation\n",
    "    save_strategy=\"steps\",                 # Save model after every epoch (or steps)\n",
    "    push_to_hub=False,                       # Avoid pushing to hub (unless needed)\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "class PerplexityCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.perplexity_by_epoch = []\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics and \"eval_loss\" in metrics:\n",
    "            perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "            self.perplexity_by_epoch.append({\"epoch\": state.epoch, \"perplexity\": perplexity})\n",
    "            wandb.log({\"eval_perplexity\": perplexity})\n",
    "            \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"eval_loss\" in logs:\n",
    "            perplexity = math.exp(logs[\"eval_loss\"])\n",
    "            logs[\"eval_perplexity\"] = perplexity\n",
    "            wandb.log({\"eval_perplexity\": perplexity})\n",
    "\n",
    "class ModelSaveCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Save the entire model to W&B at the end of each epoch\n",
    "        model_save_path = f\"{wandb.run.dir}/model_epoch_{state.epoch}\"\n",
    "        model.save_pretrained(model_save_path)\n",
    "        \n",
    "        # Upload the saved model to W&B\n",
    "        wandb.save(f\"{model_save_path}/*\")  # Save all files in the directory to W&B\n",
    "            \n",
    "perplexity_callback = PerplexityCallback()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[perplexity_callback, ModelSaveCallback]\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T04:26:11.647208Z",
     "iopub.status.busy": "2024-11-16T04:26:11.646793Z",
     "iopub.status.idle": "2024-11-16T04:26:11.664419Z",
     "shell.execute_reply": "2024-11-16T04:26:11.663318Z",
     "shell.execute_reply.started": "2024-11-16T04:26:11.647166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity by Epoch Fraction:\n",
      "       epoch   perplexity\n",
      "0   0.106180  3809.893671\n",
      "1   0.212359  2232.539624\n",
      "2   0.318539  1545.979356\n",
      "3   0.424719  1164.110886\n",
      "4   0.530898   923.151720\n",
      "..       ...          ...\n",
      "89  9.556169   153.878794\n",
      "90  9.662349   153.738638\n",
      "91  9.768528   153.681981\n",
      "92  9.874708   153.522530\n",
      "93  9.980888   153.486444\n",
      "\n",
      "[94 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerplexity by Epoch Fraction:\")\n",
    "perplexity_matrix = pd.DataFrame(perplexity_callback.perplexity_by_epoch)\n",
    "print(perplexity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"LLM_model_telugu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T04:28:19.493484Z",
     "iopub.status.busy": "2024-11-16T04:28:19.492822Z",
     "iopub.status.idle": "2024-11-16T04:28:21.127016Z",
     "shell.execute_reply": "2024-11-16T04:28:21.125284Z",
     "shell.execute_reply.started": "2024-11-16T04:28:19.493443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <bos> శ్రీకాకుళం జిల్లాలో గిరిజన ప్రజలు తమ జీవనశైలిలో వినియోగించే సంప్రదాయ పద్ధతులు, వాటి వినియోగాలు, మరియు చారిత్రక విశిష్టతలు\n",
      "Generated Text: శ్రీకాకుళం జిల్లాలో గిరిజన ప్రజలు తమ జీవనశైలిలో వినియోగించే సంప్రదాయ పద్ధతులు, వాటి వినియోగాలు, మరియు చారిత్రక విశిష్టతలు తదితర అంశాలపై ఆధారపడి ఉన్నారు. ఒక మహిళ మూడు రోజుల పాటు 'నో' అనే పేరుతో నాలుగు వర్గాల నుంచి  వరకు విస్తరించినప్పటికీ ప్రతి సంవత్సరం దాదాపు ఏడాదిపాటు, ఈ ప్రాంతంలో అన్ని ప్రాంతాల్లోనూ కొత్త తరంగాలు నిర్వహిస్తారు. ఆ రెండు అంశాల కంటే భిన్నంగా ఉన్నందున, ఈ ప్రక్రియలో కొన్ని మార్పులు, చేర్పులు చేసేవారు ప్రత్యేక శ్రద్ధ వహిస్తారు. రాష్ట్ర ప్రభుత్వం ఏర్పాటు చేసిన నూతన సచివాలయాలను కలెక్టర్లను కలిసి సమన్వయం చేసుకుని, వారి ఆరోగ్యంతో కూడిన వైద్య సలహాలు, సూచనలు అందించారు. తద్వారా ఆయా జిల్లాల్లోనూ ప్రభుత్వ అనుమతి లేకుండా ప్రజలకు సేవలు అందిస్తున్నది. జిల్లాల వారీగా పోడు భూముల సమస్య గురించి తెలుసుకున్న అధికారులంతా ఈ విషయంలో ముందుకు రాకూడదని అధికారులు చెప్తున్నారు. అలాగే ఇతర ప్రాంతాలకు కూడా నిధులు విడుదల చేయాలని సంబంధిత అధికారులకు, మండల విద్యాధికారిలకు సూచించారు. ఇందుకు సంబంధించిన పూర్తి వివరాలు ఇలా ఉన్నాయి.  \n",
      "\n",
      "Prompt: <bos> ఆంధ్రప్రదేశ్‌లోని చారిత్రక ప్రదేశాలు మరియు వాటి చరిత్ర, గుట్టల మీద కట్టబడిన దేవాలయాలు\n",
      "Generated Text: ఆంధ్రప్రదేశ్లోని చారిత్రక ప్రదేశాలు మరియు వాటి చరిత్ర, గుట్టల మీద కట్టబడిన దేవాలయాలు  వ సంవత్సరం ఆడుతున్నాయి. ఇప్పుడు ఏకంగా ఓ మసీదులో విగ్రహాలు కనిపిస్తున్నాయి. \n",
      "\n",
      "Prompt: <bos> గోదావరి నది పరివాహక ప్రాంతాలలో రైతులు సాగుచేసే పంటలు, అవి ఎటువంటి పద్ధతిలో సాగుచేయబడతాయి\n",
      "Generated Text: గోదావరి నది పరివాహక ప్రాంతాలలో రైతులు సాగుచేసే పంటలు, అవి ఎటువంటి పద్ధతిలో సాగుచేయబడతాయి.. తద్వారా రైతులు నష్టపోతున్నారు. జిల్లా కలెక్టర్ కార్యాలయంలో ఉన్న ఆర్డీ ప్రసాద్ కూడా ఈ పనులు పూర్తవుతున్నాయి. అనంతపురం జిల్లాలో సుమారు  కిలోమీటర్ల మేర సాగునీరు సరఫరా జరుగుతోంది. ఇందులో రెండు లక్షల క్యూసెక్కుల నీరు ఉంది. ఎడమవైపు నుంచి రూ. వేల కోట్లకు పైగా నీరు వస్తుంది. దీంతో ఈ ప్రాజెక్టుకు సంబంధించి మొత్తం  , ఎకరాల్లో, ఇప్పటి వరకు  . టీఎంసీల నీటి సామర్థ్యంతో నిర్మించినప్పటికీ ఇప్పటివరకు రూ. వేలు మాత్రమే మిగిలి ఉన్నాయి. చిత్తూరు జిల్లాకు సంబంధించిన ప్రతిపాదనలు త్వరలోనే వెలువడనున్నాయి. గతంలో కడపలో ఏర్పాటైన రాష్ట్ర ప్రభుత్వం గత సంవత్సరం నుంచే రైతుల కోసం ప్రత్యేకంగా ఓ కమిటీని ఏర్పాటు చేసింది. ప్రస్తుతం రాష్ట్రంలో దాదాపు సగం కోట్ల రూపాయల మేరకు ప్రాజెక్ట్ను నిర్మిస్తున్నారు. ఈ నిధులను విడుదల చేసి పూర్తి స్థాయిలో నిధులు సమీకరించాలని రైతులకు ఆయన సూచించారు. అయితే జిల్లాలో ప్రధానంగా భూగర్భ జలాలు అధికంగా ఉండటంతో పాటు మరికొన్ని వ్యవసాయ ప్రాజెక్టుల్లో అత్యధిక ప్రాధాన్యత ఇవ్వాలని తెలంగాణ ప్రజలు కోరుతున్నారు. కాగా ముఖ్యమంత్రి చంద్రశేఖర్ రావు గురువారం మహబూబ్నగర్ జిల్లాకు చెందిన రైతు వేదికలు సైతం ఏపీ ప్రజలకు అందుబాటులో ఉండడంతో రైతులు ముందుకు వస్తున్నారు.  - ఆర్థిక సంవత్సరానికిగాను రాష్ట్ర ప్రభుత్వం ప్రకటించిన నేపథ్యంలో రైతులు తమ పంటలను కొనుగోలు చేయాలని సూచనలు చేస్తున్నారు. \n",
      "\n",
      "Prompt: <bos> భారతదేశం యొక్క వాతావరణ మార్పులు రైతులకు కలిగించే ప్రభావాలు, ఆ ప్రభావాలను తగ్గించడానికి చేపట్టిన చర్యలు\n",
      "Generated Text: భారతదేశం యొక్క వాతావరణ మార్పులు రైతులకు కలిగించే ప్రభావాలు, ఆ ప్రభావాలను తగ్గించడానికి చేపట్టిన చర్యలు ప్రపంచ దేశాలకు బాగా తోడ్పడ్డాయి. * లక్షల కోట్ల సంవత్సరాల క్రితం వరకు రైతులు పంటలు పండించుకునే వ్యవసాయ విధానాలను గురించి తెలుసుకుంటూ, ప్రకృతి సేద్యంలో నీటి వినియోగం కోసం ఉపయోగించే ఎరువులు, విత్తనాలు లేదా ఇతర విత్తనాల ద్వారా రవాణా చేయవచ్చు.  - శాతం కంటే తక్కువ  రెట్లు ఎక్కువగా ఉంటే దీని వల్ల భవిష్యత్తులో పంట నష్టం వాటిల్లితే ఈ సమస్యను పరిష్కరించవచ్చు. * కేంద్ర ప్రభుత్వం కూడా వ్యవసాయ ఉత్పత్తులను ఉత్పత్తి చేయడానికి చాలా ముఖ్యమైన సమస్య అని చెప్పవచ్చు. వ్యవసాయానికి అనేక రకాల నీరు అందించడమే కాకుండా సాగు చేసిన పంటల నుండి అవసరమైన నీటిని అందిస్తుంటాము. * వ్యవసాయ ఉత్పత్తులు పర్యావరణానికి హాని చేసే పంటలను కాపాడటానికి రైతులకు అవసరమైన శక్తిని అందిస్తాయి. ఫలితంగా రైతుల ఆకలి, నష్టాలు, మంచి ఆహారం మరియు ఆరోగ్యకరమైన ఆహార పదార్థాలను తీసుకోవడంలో ఈ నిర్ణయం తప్పనిసరి. * ఒక సంవత్సరం తర్వాత రైతుకు సంబంధించినంతవరకు కిసాన్ పథకం ద్వారా భూమి మార్పిడి చేసుకోవడం వంటి అనేక ఆరోగ్య ప్రయోజనాలను పొందవచ్చు. ** వ్యవసాయంః * వ్యవసాయం, వృత్తి ఉత్పత్తుల కోసం  ఏళ్లు పైబడినవారు సంవత్సరానికి రూ. . కోట్లతో పెట్టుబడి పెట్టాలి. ఈ సమయంలో రైతులకు సరైన ఆహారం అందించేందుకు  రోజుల పాటు ప్రణాళిక సిద్ధం చేసి రైతులకు సహాయం చేయడం మంచిది. * రైతు బీమా ప్రీమియంతో పాటుగా పశువైద్యం (varimage source-struction): * రైతుల రుణాలు మొత్తం రైతులకి కేటాయించిన నిధులను తిరిగి చెల్లించాలి.  రోజులు మించి ఉంటే అవి అన్ని వర్గాల వారికి అందేలా చూడలేని పరిస్థితులు ఏర్పడుతుంటాయి. * వ్యవసాయంలో కొన్ని రకాలైన పోషకాలను కలుపుతూ ఉంటాయి. అయితే, ప్రభుత్వ రంగంలోని పశువులకే కాకుండా, వ్యవసాయ రంగానికి సంబంధించి రాష్ట్రప్రభుత్వం సబ్సిడీలు అందిస్తోంది. * ప్రభుత్వ ప్రధాన కార్యదర్శి ఆదిత్యనాథ్ సింగ్ ఓవెన్ ఈ పథకం కింద రాష్ట్రంలో రైతులకు సుమారు  , నుంచి  వేల మంది కౌలు రైతులను సురక్షిత ప్రాంతాలకు తరలిస్తారు. కౌలురైతులకు ఎక్కువ ధరల్లో లభించే అవకాశం కూడా ఉంటుంది. కాబట్టి రైతులు తగిన లాభాలు పొందేందుకు వెసులుబాటు కల్పిస్తే ప్రయోజనకరంగా ఉంటాయి.  శాతం లోపు రైతులకు ఎకరానికి ఐదు టన్నుల ధాన్యాన్ని మాత్రమే వాడతారు. వ్యవసాయ రంగంలో ప్రతి రోజూ కూలీలను తీసుకోవాలి. కానీ వ్యవసాయంతో సమానంగానూ ఉండాలంటున్నారు వ్యవసాయ మార్కెట్ యార్డులు, పరిశ్రమల నిర్వహణ రంగాలైన మార్కెటింగ్ కంపెనీలు మన దేశానికి అత్యంత ప్రాధాన్యతనిచ్చాయి.. వ్యవసాయ ఆధారిత, వాణిజ్య అవసరాల వలన ఎన్నో రంగాల్లో వృద్ధి చెందడం కష్టమవుతుందనే నమ్మశక్యం కారణంగా ఎంతో మేలు జరుగుతుంది. రైతులు చేస్తున్న సేవలు దేశానికే ఆదర్శంగా నిలిచిపోత నేపథ్యంలో ప్రభుత్వాలు ఇప్పటికే నిర్దేశించిన ముడి పదార్థాలపై ఆధారితం నిర్దేశిత పంటలకు ఎంతగానో దోహదం చేస్తాయి. దేశ ప్రజల అవసరాలకు అనుగుణంగా నాణ్యమైన విత్తనాలు, ఎరువులు అందించడం వంటివి ఆర్థిక వ్యవస్థ పరంగా అమలు చేస్తున్నవి. ముఖ్యంగా గ్రామీణ మంచినీటి సరఫరాలో రైతాంగ ప్రయోజనాలు మరియు పాడిపంటలకు ప్రయోజనం చేకూరనుండటం కలిగిఉత్పత్తి వనరుల రూపంలో ఉత్పత్తి చేస్తారు. సీజన్లలో పంటలను ప్రోత్సహించడం ద్వారా వచ్చే కరెంటు, రైతులకు ఉపయోగించుకోవచ్చు. అలాగే అధికంగా ఉన్న వ్యర్థాలతో కూడిన మురుగునీటి శుద్ధి చేయబడుతుంది. దీంతో ప్రతి  నెలలకు ఒకసారి నీరు త్రాగి అవసరమైనప్పుడు ఇది అవసరమవుతుంది. అధిక దిగుబడిని అందించే సామర్థ్యం తక్కువగా ఉన్నందున ఇది పని విషయంలో సక్రమంగా అందుబాటులో ఉంది. ఈ ఏడాది వర్షాధారమైనటువంటి తెగుళ్లు నాటి పంటకు అనుకూలంగా మారతప్పులు సమకూర్చుకుంటున్నాయి. వీటి మధ్యతరగతులుగా మారిపోతేనే మిగిలిపోతారు. *  సంవత్సరాలలో  సంవత్సరాలు కాని వారు దాదాపు ఒక లక్ష్యానికి  రోజులే కాకుండా ప్రకృతి సంపదను రక్షించుకోవడం మీద ఆధారపడితేరగదిచ్చేందుకు ఖర్చు. మొక్కలపై భౌతికంగా పనిచేసేలా వర్తించేలా ఖర్చుల్ని కొనుగోలుదారుడితో పోలిస్తే చాలా ఎక్కువ శాతం చెరకు దిగుబడిపై పెరిగే గోధుమలన్నీ ఎగుమతి చేసే పంటల్లో పుష్కలంగా లభిస్తాయి. వివిధ రంగాలకు చెందిన వ్యవసాయ ఉత్పత్తులకు మద్దతు ధర ద్వారా అధిక దిగుబడులు వస్తాయి. రైతులకు ప్రత్యామ్నాయంగా వాడే విత్తనాలను సద్వినియోగం చేసుకోవాలి. మార్కెట్లో గోధుమలు, కూరగాయలు, పండ్లు, పత్తి సాగు లాభదాయకం వాతావరణాలలో లభిస్తుంది. ప్రస్తుతం నిల్వ చేసుకోబోయే ఉత్పత్తులూ ఎగుమతి చేస్తున్నారు. అయిన వారి ఉత్పత్తిని పెంచడం కూడా మంచి సాగుకు తోడ్పడతాయి. అయితే ఎరువు అయినా నేలమాటిలోకి సంబంధించిన పదార్థాల తయారీకి తగిన డిమాండ్ పెరగడంతోపాటు సేంద్రీయ మార్కెట్టికే పోతున్న పంటను పెంపెర్తిలో తేయాకు అమ్మడానికి కావలసినవిదేశాలు బాగు చేయడంగా, అభివృద్ధి చెందుతున్న ధరలకే పరిమితం అవుతుంది. తద్వారా ఖరీఫ్ సీజన్లో వేరుశనగబండ్ల ద్వారా దిగుబడి వస్తుందనేది రైతులు మరింత మెరుగ్గా పనిచేస్తుందని చెప్పాలి.\\nచూర్థాయ్ జిల్లాలో పండిన పంటలకు, పురుగుమందుల సంఖ్యకు బదులుగా పంట లకు చేరడం, ఎరువులు తీసుకోవడం చేయడం కావొలకంటరిది. దీనికి సంబంధించిన పోషకాలతో పాటు రైతులు సాగు చేయాల్సినంత వరకు తీసుకునే కనీస బాధ్యత తీసుకునే సూచనలున్నాయి... గిట్టుబాటు ధరలను సకాలంలో పెంచాల్సి ఉండేవాట్యాత్తుదలపైనే ఆధారపడి, పంటలతోపాటు పొలంలో రైతులకు సహాయభూములు, సబ్సిడీకి అవసరమైన వరి, నాణ్యత రూపంలో వినియోగించవచ్చు.లు. ఇంకా సాగునీటికి కావల్సిన పదార్థాలు ధరలు పెరగడమెప్పుడూ పంపిణీ కోసం దరఖాస్తు ఫారమ్-రహిత ఎరువుల ఉత్పత్తిని నిరోధించగలవు.\\nఈవిధంగా రైతులకు మరియు ఉత్పత్తి చేయడం లేదు. ఎంత మేర అమలవు లేక రైతాంగం కు గురైతేనని, పంటల అవశేషాల వారీగా సాగుచేసేందుకు వీలుకావాలని లక్ష్యంగా పెట్టుకుని, అందరికీ అందుబాటులోకి రావబడుతున్న మేరకు పంట రాబడి పై కూడా దృష్టి కేంద్రీకరిపోయే ప్రోత్సాహం కోసం మనం తీసుకున్న చర్యల మూలంగా ఉత్పత్తి అయ్యే అవకాశాలు ఉన్నాయి. ణిట్టూంగ్లుగా మారడానికి కారణమవుతుందని లక్ష్యం దిశగా సాగు చేయటానికి కారణం అవుతున్నాయి. అంటే అత్యధిక ధర చెల్లించి వరి పండించిన సరుకు అభివృద్ధికి తగినంతగా ఉండాలి..\\nఆనంది\\nవ్యాప్తి గాధారం కాదు...\\nపర్మహత్తులు\\nఉపయోగదాయినగా పరిగణించబడి చూడాలి.\\nఈసిచ్చిన జీవన విధానపరిశోధన సాగుదారులకు పరిహారం అందుతుంది.\\n(సు భూముల సేకరణదారులు/మిచ్చుకోవచ్చులను ఎంపిక చేసిన ధాన్యంలో\\nఅందరిదీగా మారుతుందితున్న పంటతో సరిచేసిన నారుమవ్వనున్న ఆహారానికి ప్రాధాన్యత అనేది ఉండదు..\\nఅభివృద్ధి చెందుతున్న ఇంధనమేనాడికి నష్టం ఏవిధమైన ఆటంకం ఏర్పడుతుంది.\\nకూర లేదా కాలుష్య రహిత విద్యుత్ వాడకం ద్వారా దెబ్బతిన్న ధాన్యపు రాగులు, పంటలను సమతులపంటలు, అధిక ధరకు కల్తీ చేసుకునే పంటలు దెబ్బతినవచ్చు..\\nజీవ్వాళకు అమ్ముకునే వ్యయం ..!. ఈ ప్రక్రియ ద్వారా జీవించేందుకు దారిద్యర్ధకాలు , తక్కువ మొత్తంలో కొనకళ్లపైన పంటలతో వ్యవసాయం కోసం పన్నులు, ధరలు పెరుగుదలను తట్టుకునే కాకుండా, తక్కువ ఖరీదు, రవాణా ఖర్చుల కొరకు విత్తనం సాగు చేసే ఎరువుల లభ్యత, సాగు చేయాలి. లో కోత పడుట్ల కొరవల్లుతున్నారు. అని సూచిస్తున్న పరిస్థితులకు అనుగుణంగానే సాగుదారులన్ని సరఫరా చేస్తున్నాయింటే, అధిక ధరలకు విత్తనం, విత్తనాన్ని ప్రోత్సహించడానికి అవసరమయ్యే విధానం వంటి సుక్కరద్దుగడి, ఇతర పంటలకు ఉపయోగపడవడం ఉత్పత్తి అవుతున్న పంటలను సాగు చేసేందుకు దోహదపడగలరేపుడు తక్కువ దిగుబడిని ఇచ్చేంత విస్తీర్ణంలో వ్యవసాయద్వారా తమ ఆదాయాన్ని ఎలా వినియోగించుకోవడానికి కొంతమేరకి ఆదా చేయబడతాయి. చేస్తూ సాగు చేస్తారు. ఇలా సాగడానికి అవసరమైన మందులు సైతం సాగు విస్తీర్ణం లేని పంటకు ప్రత్యామ్నాయం కొల్లాగే ఎరువులు సాగు చేసే వరి నివేదితకు ఊసరవెల్లికీ వస్తున్న ప్రమాదాలకు అడ్డుకట్ట వేయడం కోసం, అధికకాలంక, ఎరువుల కొరత తగ్గిపోరుకు సాగు చేయాలనే లక్ష్యాలేవీరులు ఉత్పత్తి చేయాలని పెద్ద ఎత్తున సాగు చేయడానికి ఖర్చు పణవడి, పంటలకు ఉపయోగపడబడి అందముంటే కాదు, ఉత్పత్తిదారుల సంరక్షణ కోసం అవసరం కూడా అందని ద్రాక్షగానే ఉండేలా చూసుకోవాలి. ఇందులో కనీస అవసరాలు కలిగిన డిమాండ్ వలన పూర్తి ఉపాధి మార్గాలను అన్వేషించాలి. కాదని తీర్చ పడకుండా చూసుకోవడమేగాక వేసేందుకు తీసుకోవాల్సిన విధంగా వినియోగిస్తున్నట్లు వేగంగా తయారు చేయడానికి ఆస్కారం లేకుండా ఉండటానికి ఉపయోగిస్తారు. కాగలవు. కాయలైన సాగు ఖర్చులు ఎక్కువగా పంటల్లోనేనికి పంట సాగు చేసేలా వినియోగించబడుతుంది. ఎరువులు చేయడంలో విఫలం ఖర్చులకు గిట్టుబాటుధర అవసరం. లాంటిది ప్రస్తుత ధర కన్నా తక్కువ స్థాయిలో రైతులకు అనుకూలించపరుచుకోవటానికి పారిశ్రామికీకరణ పద్ధతులతో వ్యవసాయం కోసం కృషి చేయవలసిన అవసరం అనేక వ్యవసాయసాగునీరు ఆదా విలువ ఉత్పత్తికి\n",
      "\n",
      "Prompt: <bos> తెలుగు సాహిత్యంలో వేదకాలం నుండి ఆధునిక యుగం వరకు జరిగిన మార్పులు, ప్రముఖ రచయితలు\n",
      "Generated Text: తెలుగు సాహిత్యంలో వేదకాలం నుండి ఆధునిక యుగం వరకు జరిగిన మార్పులు, ప్రముఖ రచయితలు అశుతోష్ పాణం : పెన్నానది\\nముంబైః తెలుగు విశ్వవిద్యాలయం  వ శతాబ్ది వార్షికోత్సవం సందర్భంగా పలు గ్రంథాలు ప్రచురించాయి. ఈ పుస్తకం సంస్కృతంలో రాసిన సాహిత్యచరిత్రకు విశేషమైన చరిత్ర కలగాలని భావించారు. ఇది ఆంగ్ల విజ్ఞాన శాస్త్రం. \n",
      "\n",
      "Prompt: <bos> తెలుగు గ్రామీణ ప్రాంతాల్లో ప్రజల జీవన విధానాలు, ముఖ్యమైన వ్యవసాయ పద్ధతులు, గ్రామీణ సంస్కృతి\n",
      "Generated Text: తెలుగు గ్రామీణ ప్రాంతాల్లో ప్రజల జీవన విధానాలు, ముఖ్యమైన వ్యవసాయ పద్ధతులు, గ్రామీణ సంస్కృతి మొదలైన వారికి అనేక రకాల సమస్యలను పరిష్కారం చేయడంలో భాగంగా ముఖ్యమంత్రి వైఎస్ జగన్ మోహన్ రెడ్డి ఈ దిశగా ముందుకు సాగుతున్నట్లు తెలుస్తోంది. ఏపీ రాష్ట్ర వ్యాప్తంగా పలు జిల్లాల్లో కొత్తగా ఏర్పడిన ప్రభుత్వ పథకాల అమలులో కీలక పాత్ర పోషించాలని సీఎం జగన్మోహన్ రెడ్డిని కోరనున్నారు. వైఎస్సార్ ఆసరా పింఛన్లపై ఎలాంటి ఫిర్యాదు అవసరం లేదని, అందుకు సంబంధించిన వివరాలను స్వయంగా సీఎం అధికారులకు వివరించనున్నట్టు సమాచారం. ఆంధ్రప్రదేశ్ నుంచి అందుతున్న సమాచారం ప్రకారం. . రాష్ట్రంలో వచ్చే ఏడాది డిసెంబర్ నెలలో రెండు నెలల్లోనే  , కొత్త పింఛన్ల పంపిణీని చేపడుతూ సంక్షేమ పథకాలు అమలు చేయనున్నట్లు ప్రభుత్వం ప్రకటించిన విషయం తెలిసిందే. అందులో భాగంగానే ఇకపై ఏ సమస్యలు లేకుండా ఉండవని స్పష్టం అవుతోంది. మరోవైపు వైసీపీ ఎమ్మెల్యే మేకపాటి గౌతంరెడ్డి కరోనా బారినపడ్డారు. శనివారం సాయంత్రం తాడేపల్లి క్యాంపు కార్యాలయంలో ఆయన మీడియాతో మాట్లాడారు. కరోనా రోగులకు వైద్యం అందించేందుకు అవసరమైన వైద్యసేవలు అందించడంతోపాటు అన్ని వసతులు కల్పిస్తామని తెలిపారు. కరోనా కారణంగా గత కొంతకాలంగా అనారోగ్య సమస్యల సమస్య తలెత్తినా ఇప్పటికీ కోలుకోలేని దెబ్బతిని అధిగమించలేదని చెప్పుకొచ్చారు. ఏపీలో గడచిన మూడేళ్లుగా కరోనాతో రోగులు బాధపడుతున్నారు. అయితే ఆస్పత్రిలో చేరిన రోగుల పరిస్థితి పూర్తిగా విషమంగా ఉంది. దీంతో ఆస్పత్రుల్లో చికిత్స పొందే అవకాశం ఉన్నందున ఆస్పత్రులకు వెళ్లొద్దంటూ వైద్యులు సూచిస్తున్నారు. దీంతో అధికారులు ఆందోళన చెందుతున్నారు. కొన్నిరోజులుగా ఆస్పత్రి సిబ్బంది కూడా ఈ విషయమై వివరణ ఇచ్చారు.  \n",
      "\n",
      "Prompt: <bos> సముద్ర తీర ప్రాంతాల్లో చేపల వేట మరియు అక్కడి ప్రజల జీవనశైలి, అందుకు ఉపయోగపడే ఉపకరణాలు\n",
      "Generated Text: సముద్ర తీర ప్రాంతాల్లో చేపల వేట మరియు అక్కడి ప్రజల జీవనశైలి, అందుకు ఉపయోగపడే ఉపకరణాలు కూడా ఉన్నాయి. కొన్ని వందల కోట్ల మంది ప్రజలు ఈ జంతువులు ఒక అరుదైన ఘనతను సాధించారు. ఆధునిక సమాజంలో అత్యంత ముఖ్యమైన లక్షణాలలో ఒకటి. అటవీ అధికారులు కూడా తమ జంతువులకు ఈ మొక్కలు అన్ని రకాల వాటిని సంరక్షించడం గురించి తెలుసుకోవటం ద్వారా ఇక్కడ తెలుసుకుందాం. నీటి ట్యాంకులు, పురుగుమందులు వంటి సూక్ష్మ జీవుల కోసం వివిధ రకాలు చాలా పెద్ద సంఖ్యలో ఉపయోగిస్తారు. వాటిలో ప్రకృతికి సంబంధించిన ఖనిజాలను, వృక్షశాస్త్రాలను కలిగి ఉంటాయి. భూమి అనేది ఒక కళ. ఇది జలవిద్యుత్ వ్యవస్థ అని, పర్యావరణ పరిస్థితులు లేదా ఇతర పక్షులను, దాని నిర్మాణం యొక్క సృష్టి అని పేరు పెట్టినప్పుడు మాత్రమే సాధ్యమే. అడవి జంతువుల కంటే వ్యవసాయంలో ఎక్కువగా ఉపయోగించబడుతుంది. వారు భూమిపై నివసిస్తున్నారు కానీ నేడు ఆఫ్రికన్ అడవులు ఉన్నాయి, వీటిలో ప్రతి  .  రెట్లు అధికంగా ఉండేవి మరియు అవి ఆమ్ల స్వభావాన్ని కలిగి ఉంటాయి. సముద్రంలోని పురాతన కాలంలో ఈ చేపలను అనేక జాతులు గుర్తించబడింది. దీని ప్రకారం, దాదాపు  సంవత్సరాల పాటు జీవరాశిలో ఉన్నందున, చిన్న చిన్న ప్రదేశాల్లో దీనిని ఆహారంగా తీసుకోవడం వలన మనకు మంచి ప్రయోజనం కలుగుతుంది అని. మేము మా పూర్వీకులు కేవలం ఒక సంవత్సరం వ్యవధిలో జీవిస్తున్నందున దానిలోని ఇతర జీవుల నుండి సులభంగా మన దేశంలో ఎక్కువ కాలం జీవించే అవకాశం ఉంది.  వ శతాబ్దం లో ప్రపంచంలోని మొదటి అతి ప్రాచీనమైన పక్షి పాయువులోనే ఉందని కొందరు పరిశోధకులు కనుగొన్నారు. అయితే వన్యప్రాణులను అధ్యయనం చేయడానికి శాస్త్రవేత్తలు ఉపయోగించిన కొన్ని పదార్థాలను తినేవారు. పురాతన ప్రపంచ దేశాలలో మొట్టమొదటి ఈజిప్షియన్లు కూడా ఉన్నారు. ఇక్కడ దాదాపు  , మంది భారతీయులు ఉన్నారని అంచనా వేశారు. అయితే,  లో, ఇది ప్రపంచంలో అత్యధికంగా ఎగుమతి చేయబడింది.  వేల మంది మహిళలు ఉత్పత్తి చేయబడిన ఆహారాన్ని తినడం అంటే ఇది చాలా కష్టం. కానీ కొంతమంది పాశ్చాత్య దేశాల జాబితాలో ఎందుకు కొనుగోలు చేసినట్లయితే, అక్కడ వారికి ఈ విషయం గురించి మాట్లాడుతున్నారు. ఉదాహరణకు, మే నెలలో సగటున  మిలియన్ టన్నుల కొద్దీ నీరు తాగవచ్చని నమ్ముతారు. కాబట్టి, భూమి యొక్క  - ఏళ్ల మధ్య వయసు వరకు ఉంటుంది. అయితే, మానవులు దానిని నాశనం చేసే సమయంలో చాలా భిన్నంగా ఉంటాయి, వాటిలో ఈ జాతుల సాంప్రదాయం యొక్క ప్రధాన కారకాల కంటే ఎక్కువగా కనిపిస్తుంది. అలాంటి ఒక వ్యక్తి ఈ రకమైన జంతుజాలం మరియు జంతువులతో పాటు, వారి చర్మ సంరక్షణతో సంబంధం కలిగి ఉంది మరియు మీరు వాటి ఆరోగ్యాన్ని ప్రభావితం చేస్తుంది. కానీ, మానవ శరీరం లో కీటకాలు \"భూములు\" అనే తేడాను సూచిస్తుంది. ఈ ఒక ఆసక్తికరమైన కోణాన్ని కనుగొనబడింది, ఎందుకంటే అది సహజ పోషకాలను అభివృద్ధి చెందకుండా ఉంటాయి లేదు. ఈ జాతులు గాలి కాలుష్యం గణనీయంగా తగ్గింపులతో అభివృద్ధి చేయవచ్చు. అందువలన, సాధారణంగా జంతువులు అధిక మరియు మంచు మరియు మొక్కను కలిగి ఉంటుంది కాదు. ఎందుకంటే ఏ ప్రాంతంలో పెరిగే వృక్షాలు. నిజానికి, ఇది ఆహార పదార్థాల వినియోగం యొక్క కూర్పులలో ఒకటి, కాబట్టి విస్తృతంగా ఉపయోగిస్తారు జన్యువు వివిధ రసాయన పదార్ధాలతో తయారు చేస్తారు. మరియు జంతు వైవిధ్య ఎంపికల్లో ఉపయోగించే విత్తనాలు మరియు జంతువులలో ఇవి వారి ఆహారంలో చేర్చాయి చూడవచ్చు. అదనంగా, తరచుగా సంప్రదాయికణ వైవిధ్యం మరియు అనేక లక్షణాలను కలిగి ఉన్నాయి వివిధ వర్ణబాంధవాాలతో కలిసి తిత్తులు ఉన్నాయి. మరియు జంతుజాలం యొక్క కణాలు ఈ జీవులు ఒక స్థిరమైన పదార్థం. రోమంగలం మరియు జంతువులను యొక్క జంతువులు, మరియు జంతువులకు మరింత ప్రమాదకరమైన జంతువులు, ముఖ్యంగా జంతువులు ఒక ప్రత్యేక జీవకోటికి వ్యాప్తి కోసం వారు తాము అవసరమైన వాటిని ఉపయోగిస్తున్నారు. కానీ కొన్ని దేశాల్లో చాలా అరుదైన ప్రదేశాలు, చాలా అరుదుగా ఒక చెక్క, మరియు జంతు-నిరోధక ఉత్పత్తులు ఏర్పాటు చేయబడ్డాయి. మరియు జంతువుల మాంసాహారులు. అలాగే ఆఫ్రికా రాష్ట్రాలు ప్రధానంగా జంతుజాల ఉనికి మరియు మనుగడ క్రితం కనిపిస్తోంది (మరియు ఖనిజ వనరులని, కానీ దాని కూర్పు జాతుల నుండి జంతువుల మాంసం అవశేషీయ వస్తువులు తక్కువ సేంద్రీయ ఎరువులు మరియు రసాయన ఎరువులుగా పేర్కొనవచ్చు. మరియు వారు ఆహారం సాగుదారులు ఆకర్షకవి. చైనీస్ మాంసం జాతులు లేదా గోధుమ మరియు జంతు సమూహ. జంతువులకు పైగా. కానీ ప్రపంచవ్యాప్తంగా డిమాండ్లను వారి సొంత జాతుల జాతుల నుండి ఏండ్లు పెంపకంలకు సంబంధించి ఉంటుంది, కాదు పాలైవ్ది చాలా పశువైద్య జంతువులు మరియు జంతువులు నివసించే పక్షుల పెంపకందరులు  సంవత్సరాలలో వారు పెద్దవిగా కనిపిస్తాయి. ఇప్పుడు గ్రహం తయారుగా పరిగణించబడుతున్న కాన్స్ డిటిగా, భూమి నుంచి భూగోళం యొక్క జంతువులు లేదా గడ్డిపై కనిపించే పొద్దంతా గుత్తరాలితో సహా ఇతర ప్రాంతాలలో నివసించే జంతు సంపద. ఇటువంటి పక్షులు జాతి జంతుజాలం యొక్క సాధారణ జాతి జంతువుల చర్మం జంతువులు తమను వివిధ పంటలు మాంసం చాలా అరుదుగా వివిధ జాతులు సంభవిస్తాయి. ప్రకృతికి ఒక కారణం ఉన్నాయి. ఈ జంతువులు వివిధ జంతువులు పండు కోసం ఉత్తమ పంటగా భావిస్తారు. వాస్తవానికి, చాలా తక్కువ ఉష్ణమండల వాతావరణం లో వసంత వాతావరణ పరిస్థితుల ఆధారంగా మారుతూ ఉంటుంది, - కొన్ని జాతుల. వారు తక్కువ ధరలేగే చెట్లు సంఖ్య కంటే సాధారణంగా పెరుగుతాయి వాస్తవం ఉన్నప్పటికీ, ఎక్కువగా ఉండటం లో  లోహాలు ఒకటి ఉన్నాయి. కాబట్టి జంతువులైన కానీ వారు కనీసం, దాని ఆకులు మరియు జంతువుల జాతులు వాతావరణంలో ప్రతి ఒక్కరూ వివిధ కారణాలన్నీ ఒక అద్భుతమైన ఉపయోగం మీద నిరూపించడానికి చాలా ప్రసిద్ధ జాతుల వైవివ్యతిదేమియా క్యాన్సర్ మరియు విష పదార్థాలు యొక్క ప్రభావ ప్రాముఖ్యత కూడా ఒక ప్రముఖ లక్షణాలు మరియు జంతువులు, వేడికాలుగా వర్గీకరించపడే, రెండు సంస్కృతులను ఉపయోగిస్తారు. వారు ఏ వయస్సుగల ముందు ఉంటే, అటువంటి జంతువులు వారి శరీరానంగా ప్రకృతి యొక్క ఉనికికి విరుద్ధంగా పెరగడం ఉన్నప్పటికీ, మరియు చాలా వేగంగా పెరుగుతుంది, మరియు ప్రకృతి, జంతు జాతుల జాతులు చాలా తీవ్రమైన జీవులు వారి చర్మం కోసం ఎక్కువగా ఉపయోగిస్తారు. కాబట్టి అనేక జాతులు పెద్ద మరియు జంతు నాగరికత రూపాన్ని కారణంగా ఉంది. అవిః మానవజాతి జంతువుల ఆకులు మరియు చేపలు కలిగి ఉంటాయి, ఉన్నాయి. వారు చాలా అసాధారణ మరియు జంతుజాలం కలిగి ఉన్నాయి సహజ ప్రకృతిని ప్రకృతికి తిరోగదానాల్లో పునరుత్పత్తి వైవివసరాలు కూడా ఉన్నాయి. జంతుజాలం లేదా జంతువుల్లో లేదా రసాయన అవశేషాల పై దృష్టి సారించబడుతుందని ఎక్కువగా ఉన్నాయి. వారు బాగా ప్రభావితమవుతాయి, ఒక రహస్య మరియు జంతువుల యజమానులు సృష్టించ జంతువులకు హాని చాలా తీవ్రమైన జీవుల తో గొప్ప వైవిధ్యాలు ఈ రూపంలో ఎక్కువగా వివిధ రకాల జంతువుల మాంసాహారుల్లో ఈ జాతులు, మతం. prayes అని పిలుస్తారు? - ఈ సహజ పదార్థం పెరుగుదల. సాంకేతిక, పర్యావరణం మరియు జంతువులు, పర్యావరణం కాదు, ఒక సూక్ష్మజీవి వివిధ జంతు కాలుష్య కారకాలు కలిగి ఉంటుంది. మనం ప్రకృతి మొక్కల తోటమాలిగా పిలువ లోహ పరిశ్రమ జాతులు వేరుని ఉత్పత్తి చేయడానికి వారి ఆరోగ్యానికి హాని ఉంది. అయితే, తక్కువ ఉత్పాదక శక్తి బరువు కోసం హానికరమైన జంతువుల వాసనలు లేదా ఇతర ఉత్పత్తుల వినియోగం (మిట్లు) ఈ సంఖ్య స్థానంలో మీ పక్షి, మరియు జంతువుల యొక్క స్థానిక పండ్లు కంటే చాలా వివిధ రకాల. ఈ పోషక పదార్ధాలు మానవ రూపంలో ఉనికిలో ఉన్నాయి. జంతువులు యొక్క నాణ్యత ఒక పదునైన, దాని కంటే వారి మొత్తం రుచి యొక్క మూల జాతుల మాంసకృచ్ఛారణాల వాడకం లేదా సంతులత, వృక్ష జాతులు కలిగిన మట్టి నుండి ఒక అయస్కాంత పద్ధతుల ఉనికిని ఏర్పడడం మరియు జంతువుల వివిధ జంతువులను వేటాడే వివిధ నివాసులు యొక్క రకాలు కనిపిస్తాయి. జంతుశాస్త్రం తినే వివిధ. మరియు జీవ జాతికి చెందినవారు చాలా జాగ్రత్తగా నిల్వ చేయాలి.ల మధ్య అంతరావణారేటొగుడ్డగోపాతకంటెరి, అంతరించిపోయిన వృక్ష శాస్త్రం, చాలా ప్రజాదరణ మరియు జీవ జాతులు కూడా దీర్ఘ జంతువుల్లో. ఉన్నాయి. అని దాని మధ్య జీవి ఏ జాతుల కోసం ఉపయోగిస్తారు. లేదా విహారం జాతుల ప్రభావం ఆహారం లో సంభవించే అన్ని జాతులకు భిన్నంగా ఉంటాయి. కాబట్టి వారు కొవ్వు కణజాలాలకు, వారు కూడా ఉన్నాయి. మరియు ప్రోటీన్ యొక్క సరైనదిగా ఉద్భవాలకు వివిధ రంగులు చాలా తరచుగా మానవ శరీరంలో ఉన్న ఆహార వ్యర్థాలు కోసం ఒక నిర్దిష్టంగా వాడడం, అలాగే రంగు మారముట్లు, తినడానికి ఇష్టపడే, అది వివిధ రంగులలో ఇతర జాతుల కంటే ఆరోగ్యకరమైన పక్షి మొక్కలు. లో ఉత్పత్తి చేయవచ్చు, మరియు కృత్రిమ ఉష్ణోగ్రతయాలు మరియు పశువుల మాంసం యొక్క జంతువుల పదార్ధాలని రక్షిస్తారు మరియు జంతువుల. వారు సహజ పదార్ధాలుగా విభజించబడ్డాయి. వారు జంతువుల యొక్క స్వభావం అంటారు. లేదా గడ్డి మూలం నుండి పొందిన. కానీ జంతువులు పెరుగుతున్న గాలి, ఆకుపచ్చ. వారు బూడిద పుష్ప జాతులు మరియు ఆకు\n",
      "\n",
      "Prompt: <bos> తెలంగాణలో ప్రసిద్ధమైన బోనాల పండుగ మరియు దాని చరిత్ర, ఉత్సవాల సమయంలో జరిగే ప్రత్యేక కార్యక్రమాలు\n",
      "Generated Text: తెలంగాణలో ప్రసిద్ధమైన బోనాల పండుగ మరియు దాని చరిత్ర, ఉత్సవాల సమయంలో జరిగే ప్రత్యేక కార్యక్రమాలు భారత వజ్రోత్సవాలను ఘనంగా జరుపుకునే అవకాశం ఉంది. బతుకమ్మ వేడుకలకు సంబంధించి తెలంగాణ రాష్ట్ర ముఖ్యమంత్రి కేసీఆర్ ఆధ్వర్యంలో ఏర్పాట్లు జరుగుతున్నాయి. ఈ కార్యక్రమానికి ఉమ్మడి జిల్లాలో సీఎం కేసీఆర్ అన్ని రకాలుగా స్వాగతం పలికారు. అలాగే  మంది భక్తులు మేడారం జాతరను సందర్శించి, ఊరేగింపులో పాల్గొంటున్నారు. ఇందులో భాగంగా ఈ నెల  న శ్రీ రాజేశ్వరస్వామిని దర్శించుకునే మహాశివరాత్రి రోజున అమ్మవారికి రెండు రోజులపాటు ఉత్సవాలు నిర్వహిస్తారు. జాతర సందర్భంగా ప్రతి ఒక్కరూ కోలాటాలు, మంగళహారతులతో పాటు ఉత్సవ కమిటీ కూడా ఏర్పాటు చేశారు. కార్యక్రమంలో వివిధ శాఖల అధికారులు పాల్గొన్నారు. గురువారం ఉదయం అమ్మవారిని కొలిమిచేందుకు మంత్రి తలసాని శ్రీనివాస్ యాదవ్ దంపతులు పూజలు చేయించారు. అనంతరం ఆలయ పూజారులు పుష్పశ్రీ, గోస్వామి వారిని ఆశీర్వదించారు. అమ్మవార్లకు కల్యాణ మండపంలో భక్తులకు పెద్దపీట వేశారు. ఆలయంలో ఉన్న కల్యాణ ప్రాంగణం, ఆభరణాలు, లడ్డూ ప్రసాదం అందజేస్తారు. నేడు సాయంత్రం నుంచి ప్రారంభమైన శుభోత్సవం ఉగాది రోజు తెలుగు రాష్ట్రాల నుండి నేటి వరకు జరగనున్నాయి. ఈ నేపథ్యంలో వచ్చే నెలలోగా సంక్రాంతి పర్వదినాన్ని పురస్కరించుకొని ముఖ్య అతిధి పధ్నాలుగు రోజుల పాటు నిర్వహించే బ్రహ్మోత్సవాలు అంగరంగ వైభవంగా జరుగుతాయి.  \n",
      "\n",
      "Prompt: <bos> పల్లె జీవనశైలి గురించి తెలుగు సాహిత్యంలో పొందుపరచిన వివరణలు, వాటి ప్రాముఖ్యత\n",
      "Generated Text: పల్లె జీవనశైలి గురించి తెలుగు సాహిత్యంలో పొందుపరచిన వివరణలు, వాటి ప్రాముఖ్యతపై నేటి యువతరం -తెలుగు, హిందీ, ఇంగ్లీషు, హిందీ మాట్లాడే భాష ఇలా వివిధ భాషా కథలు ఉన్నాయి. ఈ రెండింటిలో ఏది కూడా చాలా ముఖ్యమైన పాత్ర. ఒక విషయం గుర్తుంచుకోవాలి. అందులో ఒకటి రెండు భాషలలో మొదటి ప్రాధాన్యతా పదాలు ఉండాలి. అవి ఒకే ఒక్క మాట లేదా నాలుగు కథల నుంచి ఎక్కువ ప్రాధాన్యం ఉంటుంది. రెండవది \"జంబ\" అంటే 'కథ' అని అర్థం. ఈ రెండూ ఒకటే! కాని కొన్ని అంశాలూ కొత్త వాటిని కూడా పరిగణనలోకి తీసుకోలేకపోయాయి. వీటన్నిటినీ సమానంగా పరిశీలిస్తే ఆ విధంగా చెప్పాలంటే (కవిత్వాని) అర్థంలో ఉంది. అయితే, ఈ విషయాన్ని మరో భాషలో చెప్పబడేవి ఆంగ్ల పదాలతో కలిపి  పేజీలుంటాయి. అయితే తెలుగులో ఇది రెండోది. నిజానికి ఇందులో ఇంగ్లిష్ లో రాసినవే. మన భాషలోని పదాల అర్థం ఏమిటి? \n",
      "\n",
      "Prompt: <bos> కృష్ణా నది తీరప్రాంతం గొప్ప వాణిజ్య కేంద్రంగా ఎలా ఎదిగింది, ఆ ప్రాంతంలో జరుగుతున్న వాణిజ్య కార్యక్రమాలు\n",
      "Generated Text: కృష్ణా నది తీరప్రాంతం గొప్ప వాణిజ్య కేంద్రంగా ఎలా ఎదిగింది, ఆ ప్రాంతంలో జరుగుతున్న వాణిజ్య కార్యక్రమాలు చాలా విలువైనాయి. ఎందుకంటే అవి సముద్రాల కంటే మెరుగైన, అద్భుతమైన నగరం. అలాగే  లో భారతదేశంలో అత్యధిక భూభాగాలు మరియు ఇతర దేశాల నుండి వలస వచ్చారు. ఈ నగర ప్రజలు తమ సొంత ఆస్తులను కలిగి ఉంటారు కాబట్టి ఇది నిజమైన చారిత్రక వారసత్వానికి సంబంధించినది, కానీ ఇక్కడ వారు ఎక్కువగా ఉన్నారు. అయితే, ఒక చిన్న పట్టణంలో ఉన్న ఈ పట్టణం అనేక అందమైన ప్రాంతాలను కలిగి ఉంటుంది. ఇప్పుడు మనం దాదాపు  మిలియన్ల యూరోల నుంచి ఇక్కడకు చేరుకుంటుందా లేదా అనే దాని గురించి మాట్లాడుతున్నాం. నేను ఇక్కడ తన పేరు వినలేదుః శ్రీ సిహెచ్. విద్యాసాగరరావు ( \"ప్రజా రవాణా మంత్రి డాక్టర్ శ్రీమతి ముందర\") యొక్క చరిత్రలో, టర్కీలో అత్యంత ప్రసిద్ధ ప్రదేశంగా ప్రసిద్ధి చెందిందిః \". డా. Mr. Srinivasa Rao: Goudsky హంగేరి యొక్క పశ్చిమ ప్రాంతం, ఇజ్మీర్లోని వివిధ ప్రాంతాలలో నివసించే పట్టణాలలో మన పొరుగు వారి నివాసం మరియు చుట్టుపక్కల గల ప్రాంతం మరియు చుట్టుపక్కల ప్రాంతాల ప్రజల నివాసాలతో పాటు, మా పౌరులు కూడా ఇక్కడ నివసించేందుకు ఇక్కడికి రావడానికి ముందుండి మరియు మాకు పెద్ద ఎత్తున గౌరవార్ధం నిర్మించబడింది. ప్రతి ఒక్కరూ నా ప్రియమైన వారిని ఆశీర్వదించాలని కోరుకుంటారు, \"అని అతను చెప్పాడు. \"మేము స్థానిక ప్రజలకు మరియు పౌరుల రక్షణతో కూడిన సేవగా పనిచేస్తాము. మేము అన్ని రకాల కార్యక్రమాలను ప్రారంభిస్తాము. మేము మా దేశం మొత్తం ప్రజలు తమ స్వంత నగరాన్ని సందర్శించడానికి అనుమతించబడలేదు. టర్కీ రాజధాని నగరంతో పాటు ప్రపంచంలో అతిపెద్ద విమానాశ్రయంగా ఉంది. మనం ప్రపంచంలోని ప్రధాన నగరాలైన ఈజిప్ట్ అని నేను నమ్ముతున్నాను. టర్కీలో భారతదేశం అతిపెద్ద నగరంగా మారింది. మీరు ఇప్పటికీ టర్కీకి దగ్గరగా ఉన్నారు మరియు ఇస్తాంబుల్లోని సుమారు ఎనిమిది వేల మంది జనాభా మరియు విదేశీ వాణిజ్యం ద్వారా అనుసంధానించబడి ఉన్నామని. మీరు మీ దేశాన్ని వదిలిపెట్టిన ఇంటి వద్ద నిలబడి ఉన్నాము. మా పూర్వీకులు కూడా మన దేశంలో ఉన్నారని నాకు తెలుసు. ఇక్కడి పర్యాటకుల సంఖ్య కూడా అదే స్థాయిలో ఉంటుందని మాకు చెప్పారు. వాస్తవానికి, టర్కీ నివాసితులైన వ్యక్తులతో మాకు ఉన్న అనుబంధాలు, శాంతి మరియు పౌర విమానయాన రంగంలో కొన్ని మార్పులు జరిగాయి. వాటిలో ఒకటి ఉంది, ఇవి కేవలం రెండు నగరాల మధ్య మంచి సంబంధం ఉంది\". \"అనేక నగరాలు మరియు పర్యాటక ప్రాంతాలకు దూరంగా ఉన్నాయిః \"ప్రపంచం ప్రపంచంలో అత్యుత్తమమైన భూభాగంగా, భారతదేశంలోని అనేక నగరాల్లో ఒకటి మరియు ఇతర ప్రాంతాలకు విస్తరించినందున మేము అభివృద్ధి చెందాలనుకుంటున్నాముంది, ఇది ఇక్కడ ప్రత్యేకమైన ప్రదేశాలలో ఒకటి. మేము సిటీ సెంటర్స్ వద్ద మా సాంస్కృతిక భవనం మరియు అన్ని హోటళ్లలో ఉన్న ప్రకృతి దృశ్యాలు ఉన్నాయి. ఈ ప్రాంతంలోని పర్యాటకులను మరింత సంరక్షించబడిన పట్టణాలను మరియు పార్క్ యొక్క పార్కులలో ఒకటిగా ప్రసిద్ధి చెందింది. అంటారియోలియన్ నుండి ది డిక్షన్ను గురించి మాట్లాడుతుంది, అక్కడ ఎవరూ పూర్తిగా అతని చుట్టూ తిరుగుబాట్ల మధ్య పురాతన నాగరికత ఉన్నప్పటికీ, మరియు చైనా లో స్థిరపడడానికి చాలా కాలం క్రితం మిగిలి ఉన్న ప్రపంచ ప్రఖ్యాత దేశవివాదిగా మారాయి.  వ శతాబ్దంలో నిర్మించిన నగరంలో  , తెలియని ప్రజలు ఎదుర్కొంటున్న, మరియు సంస్కృతి మధ్య భౌగోళిక ఆకర్షణ, చాలా ప్రాంతాల్లో ఉనికిలో ఉన్న వాటిని రక్షకసరోహితులను ఆకర్షించటానికి ఎక్కువవి. మా గ్రహం యొక్క నగరం నుండి దూరంగా ఉంది.  శాతం  మిలియన్ టన్నుల స్మారక మైదాన స్థలం ఇది. ఇది చాలా గొప్ప పర్యాటక మార్కెట్లలో, నగరానికి వెళ్ళేదివ్వరినీ కనుగొన్నారు మరియు ఆంటివేళాగిల్లులతో సహా కొనసాగుతుందిః సరస్సు దగ్గరనుండి  .రఫ్ అయిన డర్దుల్లాజ్నీనోల్ రాజు ముహమ్మద్ అస్లావుద్దీన్ యొక్క మ్యూజియంలు ఉన్నాయి. మేము దక్షిణ ఆఫ్రికా చర్చి నుండి పారిస్తావాహరులతో కలుపలను, ముఖ్యంగా కొండపై ఉన్న పాత సముద్రాన్ని నిర్మిస్తామని ఆయన మునిసిపాలిటీలున్న పట్టాలిచ్చే మరియు పర్యాటక వంతెనలను ఆక్రమించిన నగరం యొక్క ఏకైక కేంద్రంగా కాకుండా,  మిలియన్ల మందికి చెందిన కార్లు కలిగి ఉన్నారు, ముఖ్యంగా ప్రజల కోసం ఒక సైనిక బృందం ఉందని కాదు. ప్రతి సంవత్సరం చాలా విలువైన ప్రదేశాలు ఉన్నాయి, ఇది టర్కీ, ఐరోపాకు అనుగుణంగా ఉన్న ప్రాంతం అంతటా చాలా ముఖ్యమైనవి. ఇక్కడ ఎల్లప్పుడూ అద్భుతమైన చారిత్రక ప్రదేశాలను ఉన్నాయి, మరియు మేము గొప్ప పర్యాటక నగరాల జాబితాలో ఉన్నప్పటికీ బాగా ప్రాచుర్యం పొందుబాటుగా మారేవోని ప్రకటించబడింది ఇజ్రాయెల్ మరియు ఈజిప్టియన్మార్తి దేశాలు. నగరం యొక్క పురాతన నగరం - బెయిగాన్ యొక్క కళలో ఏజియన్ యొక్క నివాసిస్తాన్ నుండి ఒబెకాగుడికి వెళల్గాలనిస్తోంది! టర్కీ ప్రభుత్వంగా, మా ప్రధాన కార్యాలయ సమూహాలు ఉన్నాయి. టర్కీ మరియు ఎనేడ్కోనరేపేరును మరియు వారు ఒక ద్వీపకల్పంలో వారి స్వంత వ్యాపార కార్యకలాపాలు మరియు యూరోపియన్ ప్రజలు సంతోషంగా ఉన్నారు. మేము వారికి అన్ని భవనాలు చాలా స్నేహపూర్వక ప్రయాణాన్ని నడిబొన్నవోద్దినప్పుడు ఒక ప్రత్యేక స్థానాన్ని సృష్టించగలదిందారు వాస్తవం మాత్రమే కాకుండా, సజీవంగా ఉండే నగరాలు మరియు వారి ప్రాధాన్యతలు, ఉన్న అంతర్గత కట్టడాలకు ప్రసిద్ది మరియు మన రాష్ట్ర గుర్తింపు వంటి దేశంలోని అత్యుత్తమ సామాజిక కారిడార్లలో అత్యంత ప్రాముఖ్యత లేని స్వాతంత్ర్యం వచ్చింది, రష్యన్ మాట్లాడే, మన దేశానికి, మనసాధురీతరత యొక్క భూతాలు, ప్రపంచ వారసత్వ మరియు జాతీయ ఉద్యానవనంక్ రిపబ్లిక్లను సృష్టించడానికి, రష్యా కోటాయిగార్డ్ యొక్క అభివృద్ధికి కూడా కాకుండా ఉంటాయి. మరియు మా. మరియు కొలవారీచార్జర్హాజియాంటెప్లా మారిందిస్తున్నారు. ఈ నగరం యొక్క ఉత్తమ నగరాన్ని కనుగొనటానికి వెళ్ళండి. బుర్కేయన్నే నాదం చేసింది. Kreye పెంపకం చేయబడ్డాయి - లసివ్వాలో నిర్మించారుస్తూ - మరియు ఇటాలియన్ రైల్వేబస్సులు మీద గొప్ప సంఘటనలు మరియు కొత్త మరియు కొత్త ఓడ రేవును విస్తరించడానికి మార్చబడుతున్న ప్రదేశం. చారిత్రక ప్రాంతాలుః గాయ్ ఇమ్మంటాలో ఉండటం లేదు,  -స్తెన్నలోడి వరకు కొనసాగిపోగులుగా, కాని నగరంపై మనుగడ యొక్క నగరం యొక్క చారిత్రక రహదారి మార్గం గురించి తెలుసుకోవడానికి మరింత ఆసక్తిని పెంచారు, వారి కుటుంబాలు ఆనందించే అనేక సంవత్సరాలు కలిసి పనిచేశారు. వారు వారి స్వస్థల్పూరకట్టినీ వెనుక పడమటిగ్గాల యొక్క నివాసే  కిలోమీటర్ల వరకు ప్రవహించండి ఉంది. Eural విద్యలీష్మాన్తులను ఇష్టపడరుస్తుంది. మరియు వారు చాలా చిన్న గ్రామం అంతా అడవి మరియు పిగ్మెత్తుడిని చూయింగ్. నగరం యొక్క అందం యొక్క చరిత్ర ఉంది, మరియు మా సమయం వరకు, అది ఒక అద్భుత కట్టడ మార్గంలో ఉంది. కానీ ఒక సహజ దళాలు తూర్పు వాతావరణం దృష్టిని ఆకర్షింపట్లు కూడా కాదు, ఆమె మాస్కోలో తన మాజీ మరియు మేయర్ కు బదిలీ చేశారు. టర్కీ యొక్క నివాసిత గురించి అన్ని కొత్త, పర్వత శ్రేణి ప్రజలు తమ, ఒక నగరాన్ని చూసిన ప్రదర్శనల్లో అత్యంత ప్రమాదకరమైన గుప్తంగా వారు మరియు కూడా అధిక సంఖ్యలో ఒక బలమైన విజయాలు మరియు గొప్ప ప్రయాణ గమ్యస్థానానికి అందపుదిద్దే పడవు. మా అధ్యక్షుడు భాగంగా కాదు, - కానీ చాలా తెలివైన ప్రజలకు, ఇది అత్యంత వేగంగా పని చూడటం వంటి ప్రజలు చాలా మనోహరమైన వారు నిరంతరం ప్రయాణించే ప్రదేశాన్ని స్వాధీనం ముందు, చాలా మంది పర్యాటకులకు ఇది చాలా ప్రజాదరణ కోసం ఒక నగరాన్ని చూడటానికి ఇష్టపడతారు. ఈ దిశలో మరింత ఆధునిక, వినోదం, పురాతన కాలెస్ట్స్ కోసం ఒక రహస్య, మరియు uv మరియు గంతగా, స్పాన్ రైల్వేసుందరిలో ఉండటానికి సహాయం చేసింది చేసిన ప్రజలను మరియు ప్రపంచవ్యాప్తంగా నుండి నగరం గురించి తీవ్రంగా పోరాడడానికి కొనసాగించాల్సిన సేవలు అందిస్తున్నాయి. దీని పని నుండి, ఎక్కడ ఒక చిన్న కుటుంబం నుండి వేలాది మరియు చిక్కడ మరియు క్రూరాండం యొక్క వినోదం ప్రజల భాగస్వామ్యంతో జరుపుకుంటారు లేదు. ఇక్కడ వారి సందర్శకులు నిజంగా చిరస్థాయి నుండి వారి ఆసక్తి కలిగి ఉంది, ప్రజలు ట్రాఫిక్ నెట్వర్క్ కోసం వారు చాలా ఆకర్షణీయమైన తో ఉంది. మరియు వారి అభిమానులు కోసం వారు చూడటానికి. కానీ కానీ చాలా సులభంగా మరియు ప్రపంచవ్యాప్త సౌకర్యాలు మరియు యువ అతిథి. మరియు ఉన్నత స్థానాన్ని బలోపేతం చేయడానికి వారు ఉన్నారు.  స్తాబుల్లంతాపిందేలుబొమ్మైఫ్దాక్కారు, మరియు యుగంజాడల నుండి నగరంలో పార్క్-అగ్రహాన్ యొక్క అన్ని జాతులను తిరిగిరాని నగరం. \" కూడా సెయింట్ నది\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=1.0, top_k=50, top_p=0.95, repetition_penalty=1.2):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to generate text from.\n",
    "        max_length (int): Maximum length of the generated text.\n",
    "        temperature (float): Sampling temperature for diversity.\n",
    "        top_k (int): Top-k sampling.\n",
    "        top_p (float): Top-p (nucleus) sampling.\n",
    "        repetition_penalty (float): Penalty for token repetition.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompts = [\n",
    "    \"<bos> శ్రీకాకుళం జిల్లాలో గిరిజన ప్రజలు తమ జీవనశైలిలో వినియోగించే సంప్రదాయ పద్ధతులు, వాటి వినియోగాలు, మరియు చారిత్రక విశిష్టతలు\",\n",
    "    \"<bos> ఆంధ్రప్రదేశ్‌లోని చారిత్రక ప్రదేశాలు మరియు వాటి చరిత్ర, గుట్టల మీద కట్టబడిన దేవాలయాలు\",\n",
    "    \"<bos> గోదావరి నది పరివాహక ప్రాంతాలలో రైతులు సాగుచేసే పంటలు, అవి ఎటువంటి పద్ధతిలో సాగుచేయబడతాయి\",\n",
    "    \"<bos> భారతదేశం యొక్క వాతావరణ మార్పులు రైతులకు కలిగించే ప్రభావాలు, ఆ ప్రభావాలను తగ్గించడానికి చేపట్టిన చర్యలు\",\n",
    "    \"<bos> తెలుగు సాహిత్యంలో వేదకాలం నుండి ఆధునిక యుగం వరకు జరిగిన మార్పులు, ప్రముఖ రచయితలు\",\n",
    "    \"<bos> తెలుగు గ్రామీణ ప్రాంతాల్లో ప్రజల జీవన విధానాలు, ముఖ్యమైన వ్యవసాయ పద్ధతులు, గ్రామీణ సంస్కృతి\",\n",
    "    \"<bos> సముద్ర తీర ప్రాంతాల్లో చేపల వేట మరియు అక్కడి ప్రజల జీవనశైలి, అందుకు ఉపయోగపడే ఉపకరణాలు\",\n",
    "    \"<bos> తెలంగాణలో ప్రసిద్ధమైన బోనాల పండుగ మరియు దాని చరిత్ర, ఉత్సవాల సమయంలో జరిగే ప్రత్యేక కార్యక్రమాలు\",\n",
    "    \"<bos> పల్లె జీవనశైలి గురించి తెలుగు సాహిత్యంలో పొందుపరచిన వివరణలు, వాటి ప్రాముఖ్యత\",\n",
    "    \"<bos> కృష్ణా నది తీరప్రాంతం గొప్ప వాణిజ్య కేంద్రంగా ఎలా ఎదిగింది, ఆ ప్రాంతంలో జరుగుతున్న వాణిజ్య కార్యక్రమాలు\"\n",
    "]\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prompt, max_length=1280, temperature=0.8)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated Text: {generated_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <bos> గోదావరి నది పరివాహక ప్రాంతాల్లో రైతులు సాగించే పంటలు, వీటిని సాగించడానికి ఉపయోగించే పద్ధతులు, సాగునీటి సరఫరా, మరియు భూగర్భ జలాల వినియోగం అనేవి రైతుల జీవనోపాధి కోసం ముఖ్యమైన అంశాలు. ఈ ప్రాంతంలో రైతులు పంటలు సాగిస్తున్న విధానం, సవాళ్లను ఎదుర్కొనటానికి ప్రభుత్వం చేపడుతున్న చర్యలు, మరియు రాష్ట్ర ప్రభుత్వం ఇటీవల తీసుకున్న చర్యలు, పధకాలు మరియు ప్రాజెక్టుల వివరాలు ఇవ్వడమే కాదు. సాగునీటి పథకాలు, పంటల లాభదాయకత, తదితర అంశాలను కూడా. ప్రస్తుతం, రైతులు ప్రధానంగా\n",
      "Generated Text: గోదావరి నది పరివాహక ప్రాంతాల్లో రైతులు సాగించే పంటలు, వీటిని సాగించడానికి ఉపయోగించే పద్ధతులు, సాగునీటి సరఫరా, మరియు భూగర్భ జలాల వినియోగం అనేవి రైతుల జీవనోపాధి కోసం ముఖ్యమైన అంశాలు. ఈ ప్రాంతంలో రైతులు పంటలు సాగిస్తున్న విధానం, సవాళ్లను ఎదుర్కొనటానికి ప్రభుత్వం చేపడుతున్న చర్యలు, మరియు రాష్ట్ర ప్రభుత్వం ఇటీవల తీసుకున్న చర్యలు, పధకాలు మరియు ప్రాజెక్టుల వివరాలు ఇవ్వడమే కాదు. సాగునీటి పథకాలు, పంటల లాభదాయకత, తదితర అంశాలను కూడా. ప్రస్తుతం, రైతులు ప్రధానంగా వ్యవసాయం, విద్య, ఉద్యోగావకాశాలు లేక వ్యాధులు ప్రబలకుండా ఉండేందుకు తగిన చర్యలు తీసుకుంటున్నారు. జిల్లాలోని అన్ని ప్రాంతాలు ఒక చోట సాగు చేస్తున్నాయి, తద్వారా  నుంచి  వరకు రైతుల అభివృద్ధిపై శ్రద్ధ వహించాలి. ముఖ్యంగా ఉమ్మడి జిల్లావ్యాప్తంగా  వేల హెక్టార్లలో రైతులకు  . లక్షల మెట్రిక్ టన్నుల ధాన్యం కొనుగోలు కేంద్రాలను ఏర్పాటు చేయడం జరుగుతోంది. దీని వల్ల రైతులకు సంబంధించిన సమస్యలను తక్షణమే పరిష్కరించేలా ప్రభుత్వం లక్ష్యంగా పెట్టుకుంది. రాష్ట్రంలో వ్యవసాయానికి అవసరమైన విత్తనాలు, ఎరువులు అందించాల్సిన అవసరం ఉంది. ఇందులో భాగంగా, ప్రతి గ్రామంలోనే వ్యవసాయ ఉత్పత్తుల ఉత్పత్తిని పెంచేందుకు ఎక్కువ శాతం సబ్సిడీ ఇస్తుంది. గత సంవత్సర కాలంలో ఇప్పటివరకు దాదాపు రూ.  , కోట్లను మాత్రమే అందిస్తున్నది. ప్రస్తుతం మార్కెట్లో డిమాండ్ అధికంగా ఉన్న రైతులకు కనీస మద్దతు ధర కూడా లభించదు. తెలంగాణ ముఖ్యమంత్రి కేసీఆర్ ప్రకటించిన గ్రీన్ ఇండియా ఛాలెంజ్ కార్యక్రమం ద్వారా.. మొత్తం ఉత్పత్తిలో  శాతం ( - ) ను అందుబాటులోకి తెచ్చారు. దీనికి అనుగుణంగా జిల్లాలో మూడు జిల్లాల కంటే సుమారు రెండులక్షల కోట్ల నుండి  శాతం సబ్సిడీతో కూడిన రైతు బీమా ప్రీమియంను అందిస్తున్నారు. ఇక మార్చి నెలలో మొత్తం వరిసాగు పూర్తి కావడంతో రాబోయే వర్షాకాలంలో మరిన్ని నాణ్యమైన ఉత్పత్తి స్థాయిలను ప్రజలకు అందించాలన్న లక్ష్యంతో ముందుకు సాగాలని ప్రణాళిక రూపొందించారు. ఇప్పటికే పలు జిల్లాల్లో ధాన్యం సేకరణ ప్రారంభించడంతో పాటు వాటి వినియోగంపై దృష్టి సారించాలని ముఖ్యమంత్రి నిర్ణయించారు.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<bos> గోదావరి నది పరివాహక ప్రాంతాల్లో రైతులు సాగించే పంటలు, వీటిని సాగించడానికి ఉపయోగించే పద్ధతులు, సాగునీటి సరఫరా, మరియు భూగర్భ జలాల వినియోగం అనేవి రైతుల జీవనోపాధి కోసం ముఖ్యమైన అంశాలు. ఈ ప్రాంతంలో రైతులు పంటలు సాగిస్తున్న విధానం, సవాళ్లను ఎదుర్కొనటానికి ప్రభుత్వం చేపడుతున్న చర్యలు, మరియు రాష్ట్ర ప్రభుత్వం ఇటీవల తీసుకున్న చర్యలు, పధకాలు మరియు ప్రాజెక్టుల వివరాలు ఇవ్వడమే కాదు. సాగునీటి పథకాలు, పంటల లాభదాయకత, తదితర అంశాలను కూడా. ప్రస్తుతం, రైతులు ప్రధానంగా\"\n",
    "\n",
    "generated_text = generate_text(prompt, max_length=1280, temperature=0.8)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation of the above prompt and generated text in English:-\n",
    "\n",
    "\n",
    "\n",
    "### *Prompt*:- \n",
    "- \"Crops grown by farmers in the Godavari river basin, farming practices, irrigation water supply, and groundwater use are important factors for farmers' livelihoods. It not only gives details of the way farmers are cultivating crops in the region, steps taken by the government to meet the challenges, and recent measures, schemes and projects taken by the state government. Irrigation schemes, profitability of crops, etc. At present, farmers are the main ones\"\n",
    "\n",
    "### *Generated Text*:-\n",
    "- Crops grown by farmers in the Godavari river basin, farming practices, irrigation water supply, and groundwater use are important factors for farmers' livelihoods. It not only gives details of the way farmers are cultivating crops in the region, steps taken by the government to meet the challenges, and recent measures, schemes and projects taken by the state government. Irrigation schemes, profitability of crops, etc. At present, farmers are mainly concerned with agriculture, education, employment opportunities or taking appropriate measures to avoid the spread of diseases. All the regions of the district are cultivating in one place, so that the development of the farmers should be taken care of. Especially for the farmers in thousands of hectares across the joint district. Lakh metric tons of grain procurement centers are being set up. Due to this, the government aims to solve the problems related to farmers immediately. There is a need to provide essential seeds and fertilizers for agriculture in the state. As part of this, each village will be subsidized at a higher percentage to increase the production of agricultural products. During the last year so far around Rs. , offering only Rs. At present, the farmers who have high demand in the market are not getting even the minimum support price. Through the Green India Challenge program announced by Telangana Chief Minister KCR.. percent (-) of the total production has been made available. In accordance with this, farmers insurance premium with percentage subsidy of about two lakh crores is being provided in the district over three districts. With the entire paddy harvesting completed in the month of March, plans have been made to move forward with the aim of providing more quality production levels to the people in the upcoming rainy season. The Chief Minister has already started collection of grain in many districts and has decided to focus on its utilization."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6092462,
     "sourceId": 9914608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
