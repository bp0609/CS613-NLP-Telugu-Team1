{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T05:02:44.245848Z",
     "iopub.status.busy": "2024-11-22T05:02:44.245178Z",
     "iopub.status.idle": "2024-11-22T05:02:59.528537Z",
     "shell.execute_reply": "2024-11-22T05:02:59.527726Z",
     "shell.execute_reply.started": "2024-11-22T05:02:44.245809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7914727ed1aaa75b1258a7f2147f0e916a0abdab6262dc41675e67fb779cf8a0\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: portalocker, nltk, sacrebleu, rouge_score, evaluate\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 nltk-3.9.1 portalocker-3.0.0 rouge_score-0.1.2 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "! pip install evaluate sacrebleu rouge_score -U nltk requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:02:59.530665Z",
     "iopub.status.busy": "2024-11-22T05:02:59.530386Z",
     "iopub.status.idle": "2024-11-22T05:03:32.898675Z",
     "shell.execute_reply": "2024-11-22T05:03:32.897802Z",
     "shell.execute_reply.started": "2024-11-22T05:02:59.530637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, sys, json, torch, warnings\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModelForSequenceClassification, \n",
    "                        TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "from huggingface_hub import login\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disabling Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:32.900219Z",
     "iopub.status.busy": "2024-11-22T05:03:32.899747Z",
     "iopub.status.idle": "2024-11-22T05:03:32.904922Z",
     "shell.execute_reply": "2024-11-22T05:03:32.904130Z",
     "shell.execute_reply.started": "2024-11-22T05:03:32.900194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory:  /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory: \",os.getcwd())\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:32.907642Z",
     "iopub.status.busy": "2024-11-22T05:03:32.907250Z",
     "iopub.status.idle": "2024-11-22T05:03:32.942714Z",
     "shell.execute_reply": "2024-11-22T05:03:32.941999Z",
     "shell.execute_reply.started": "2024-11-22T05:03:32.907602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"Bpp06\"] = \"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\"  # Replace it with your Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:32.943935Z",
     "iopub.status.busy": "2024-11-22T05:03:32.943687Z",
     "iopub.status.idle": "2024-11-22T05:03:33.122843Z",
     "shell.execute_reply": "2024-11-22T05:03:33.121960Z",
     "shell.execute_reply.started": "2024-11-22T05:03:32.943911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=os.getenv(\"Bpp06\"), add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:33.124513Z",
     "iopub.status.busy": "2024-11-22T05:03:33.124131Z",
     "iopub.status.idle": "2024-11-22T05:03:33.128298Z",
     "shell.execute_reply": "2024-11-22T05:03:33.127539Z",
     "shell.execute_reply.started": "2024-11-22T05:03:33.124454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:33.129472Z",
     "iopub.status.busy": "2024-11-22T05:03:33.129247Z",
     "iopub.status.idle": "2024-11-22T05:03:34.582812Z",
     "shell.execute_reply": "2024-11-22T05:03:34.582096Z",
     "shell.execute_reply.started": "2024-11-22T05:03:33.129450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.getenv(\"Bp06\"))\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:34.584477Z",
     "iopub.status.busy": "2024-11-22T05:03:34.583881Z",
     "iopub.status.idle": "2024-11-22T05:03:34.634894Z",
     "shell.execute_reply": "2024-11-22T05:03:34.633858Z",
     "shell.execute_reply.started": "2024-11-22T05:03:34.584439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Parameters of Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:34.636244Z",
     "iopub.status.busy": "2024-11-22T05:03:34.635982Z",
     "iopub.status.idle": "2024-11-22T05:03:34.645780Z",
     "shell.execute_reply": "2024-11-22T05:03:34.645146Z",
     "shell.execute_reply.started": "2024-11-22T05:03:34.636221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading SQUAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:34.648634Z",
     "iopub.status.busy": "2024-11-22T05:03:34.648346Z",
     "iopub.status.idle": "2024-11-22T05:03:39.408572Z",
     "shell.execute_reply": "2024-11-22T05:03:39.407677Z",
     "shell.execute_reply.started": "2024-11-22T05:03:34.648611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "squad = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing SQUAD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:39.410241Z",
     "iopub.status.busy": "2024-11-22T05:03:39.409868Z",
     "iopub.status.idle": "2024-11-22T05:03:39.417541Z",
     "shell.execute_reply": "2024-11-22T05:03:39.416739Z",
     "shell.execute_reply.started": "2024-11-22T05:03:39.410200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_squad_data(examples):\n",
    "    inputs = [q + \" \" + c for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    ans = []\n",
    "    for answer, context in zip(examples[\"answers\"], examples[\"context\"]):\n",
    "        if len(answer[\"text\"]) > 0:\n",
    "            answer_text = answer[\"text\"][0]\n",
    "            start_idx = answer[\"answer_start\"][0]\n",
    "            end_idx = start_idx + len(answer_text)\n",
    "        else:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "            answer_text = \"\"\n",
    "\n",
    "        start_positions.append(start_idx)\n",
    "        end_positions.append(end_idx)\n",
    "        ans.append(answer_text)\n",
    "        \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs[\"start_positions\"] = start_positions\n",
    "    model_inputs[\"end_positions\"] = end_positions\n",
    "    model_inputs[\"answers\"] = ans\n",
    "\n",
    "    return {k: v for k, v in model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:39.418717Z",
     "iopub.status.busy": "2024-11-22T05:03:39.418368Z",
     "iopub.status.idle": "2024-11-22T05:03:48.558458Z",
     "shell.execute_reply": "2024-11-22T05:03:48.557642Z",
     "shell.execute_reply.started": "2024-11-22T05:03:39.418681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = squad[\"train\"].select(range(8000)).map(preprocess_squad_data, batched=True)\n",
    "test_dataset = squad[\"validation\"].select(range(2000)).map(preprocess_squad_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CustomQA Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:48.559907Z",
     "iopub.status.busy": "2024-11-22T05:03:48.559645Z",
     "iopub.status.idle": "2024-11-22T05:03:48.569361Z",
     "shell.execute_reply": "2024-11-22T05:03:48.568356Z",
     "shell.execute_reply.started": "2024-11-22T05:03:48.559882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomQAModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomQAModel, self).__init__()\n",
    "        \n",
    "        # Load configuration and base model\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        # Add a question answering head (linear layer)\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.qa_outputs = nn.Linear(hidden_size, 2)  # Start and end ind\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, start_positions=None, end_positions=None):\n",
    "        # Pass inputs through the base model\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        # Use the last hidden state for QA head\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "\n",
    "        # Split into start and end logits\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # Ignore padding positions when calculating loss\n",
    "            ignored_index = start_logits.size(1)  # seq_length\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            # Compute the loss using CrossEntropyLoss\n",
    "            start_loss = F.cross_entropy(start_logits, start_positions, ignore_index=ignored_index)\n",
    "            end_loss = F.cross_entropy(end_logits, end_positions, ignore_index=ignored_index)\n",
    "            loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        return {\n",
    "            \"start_logits\": start_logits,\n",
    "            \"end_logits\": end_logits,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "    def save_pretrained(self, save_directory):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # Save model weights\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "        # Save configuration (replace with actual configuration logic)\n",
    "        config = {\"model_name\": \"CustomQAModel\"}\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory, model_name):\n",
    "        model = cls(model_name)\n",
    "        model.base_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "        model.qa_outputs.load_state_dict(torch.load(f\"{save_directory}/qa_outputs.bin\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:03:48.570835Z",
     "iopub.status.busy": "2024-11-22T05:03:48.570443Z",
     "iopub.status.idle": "2024-11-22T05:04:51.014187Z",
     "shell.execute_reply": "2024-11-22T05:04:51.013234Z",
     "shell.execute_reply.started": "2024-11-22T05:03:48.570797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qa_model = CustomQAModel(model_name)\n",
    "qa_model.config.pad_token_id = qa_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting the Parameters of QA Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:51.015575Z",
     "iopub.status.busy": "2024-11-22T05:04:51.015287Z",
     "iopub.status.idle": "2024-11-22T05:04:51.036012Z",
     "shell.execute_reply": "2024-11-22T05:04:51.035089Z",
     "shell.execute_reply.started": "2024-11-22T05:04:51.015550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------+\n",
      "|                          Modules                           | Parameters |\n",
      "+------------------------------------------------------------+------------+\n",
      "|            base_model.model.embed_tokens.weight            | 262668288  |\n",
      "|     base_model.model.layers.0.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.0.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.0.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.0.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.0.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.0.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.0.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.0.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.0.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.1.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.1.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.1.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.1.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.1.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.1.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.1.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.1.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.1.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.2.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.2.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.2.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.2.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.2.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.2.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.2.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.2.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.2.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.3.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.3.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.3.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.3.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.3.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.3.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.3.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.3.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.3.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.4.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.4.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.4.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.4.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.4.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.4.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.4.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.4.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.4.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.5.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.5.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.5.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.5.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.5.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.5.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.5.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.5.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.5.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.6.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.6.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.6.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.6.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.6.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.6.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.6.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.6.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.6.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.7.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.7.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.7.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.7.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.7.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.7.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.7.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.7.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.7.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.8.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.8.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.8.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.8.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.8.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.8.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.8.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.8.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.8.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.9.self_attn.q_proj.weight      |  4194304   |\n",
      "|     base_model.model.layers.9.self_attn.k_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.9.self_attn.v_proj.weight      |  1048576   |\n",
      "|     base_model.model.layers.9.self_attn.o_proj.weight      |  4194304   |\n",
      "|       base_model.model.layers.9.mlp.gate_proj.weight       |  16777216  |\n",
      "|        base_model.model.layers.9.mlp.up_proj.weight        |  16777216  |\n",
      "|       base_model.model.layers.9.mlp.down_proj.weight       |  16777216  |\n",
      "|      base_model.model.layers.9.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.9.post_attention_layernorm.weight  |    2048    |\n",
      "|     base_model.model.layers.10.self_attn.q_proj.weight     |  4194304   |\n",
      "|     base_model.model.layers.10.self_attn.k_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.10.self_attn.v_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.10.self_attn.o_proj.weight     |  4194304   |\n",
      "|      base_model.model.layers.10.mlp.gate_proj.weight       |  16777216  |\n",
      "|       base_model.model.layers.10.mlp.up_proj.weight        |  16777216  |\n",
      "|      base_model.model.layers.10.mlp.down_proj.weight       |  16777216  |\n",
      "|     base_model.model.layers.10.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.10.post_attention_layernorm.weight |    2048    |\n",
      "|     base_model.model.layers.11.self_attn.q_proj.weight     |  4194304   |\n",
      "|     base_model.model.layers.11.self_attn.k_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.11.self_attn.v_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.11.self_attn.o_proj.weight     |  4194304   |\n",
      "|      base_model.model.layers.11.mlp.gate_proj.weight       |  16777216  |\n",
      "|       base_model.model.layers.11.mlp.up_proj.weight        |  16777216  |\n",
      "|      base_model.model.layers.11.mlp.down_proj.weight       |  16777216  |\n",
      "|     base_model.model.layers.11.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.11.post_attention_layernorm.weight |    2048    |\n",
      "|     base_model.model.layers.12.self_attn.q_proj.weight     |  4194304   |\n",
      "|     base_model.model.layers.12.self_attn.k_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.12.self_attn.v_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.12.self_attn.o_proj.weight     |  4194304   |\n",
      "|      base_model.model.layers.12.mlp.gate_proj.weight       |  16777216  |\n",
      "|       base_model.model.layers.12.mlp.up_proj.weight        |  16777216  |\n",
      "|      base_model.model.layers.12.mlp.down_proj.weight       |  16777216  |\n",
      "|     base_model.model.layers.12.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.12.post_attention_layernorm.weight |    2048    |\n",
      "|     base_model.model.layers.13.self_attn.q_proj.weight     |  4194304   |\n",
      "|     base_model.model.layers.13.self_attn.k_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.13.self_attn.v_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.13.self_attn.o_proj.weight     |  4194304   |\n",
      "|      base_model.model.layers.13.mlp.gate_proj.weight       |  16777216  |\n",
      "|       base_model.model.layers.13.mlp.up_proj.weight        |  16777216  |\n",
      "|      base_model.model.layers.13.mlp.down_proj.weight       |  16777216  |\n",
      "|     base_model.model.layers.13.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.13.post_attention_layernorm.weight |    2048    |\n",
      "|     base_model.model.layers.14.self_attn.q_proj.weight     |  4194304   |\n",
      "|     base_model.model.layers.14.self_attn.k_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.14.self_attn.v_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.14.self_attn.o_proj.weight     |  4194304   |\n",
      "|      base_model.model.layers.14.mlp.gate_proj.weight       |  16777216  |\n",
      "|       base_model.model.layers.14.mlp.up_proj.weight        |  16777216  |\n",
      "|      base_model.model.layers.14.mlp.down_proj.weight       |  16777216  |\n",
      "|     base_model.model.layers.14.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.14.post_attention_layernorm.weight |    2048    |\n",
      "|     base_model.model.layers.15.self_attn.q_proj.weight     |  4194304   |\n",
      "|     base_model.model.layers.15.self_attn.k_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.15.self_attn.v_proj.weight     |  1048576   |\n",
      "|     base_model.model.layers.15.self_attn.o_proj.weight     |  4194304   |\n",
      "|      base_model.model.layers.15.mlp.gate_proj.weight       |  16777216  |\n",
      "|       base_model.model.layers.15.mlp.up_proj.weight        |  16777216  |\n",
      "|      base_model.model.layers.15.mlp.down_proj.weight       |  16777216  |\n",
      "|     base_model.model.layers.15.input_layernorm.weight      |    2048    |\n",
      "| base_model.model.layers.15.post_attention_layernorm.weight |    2048    |\n",
      "|                base_model.model.norm.weight                |    2048    |\n",
      "|                     qa_outputs.weight                      |    4096    |\n",
      "|                      qa_outputs.bias                       |     2      |\n",
      "+------------------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "count_parameters(qa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Parameters before Freezing the Base Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:51.037597Z",
     "iopub.status.busy": "2024-11-22T05:04:51.037037Z",
     "iopub.status.idle": "2024-11-22T05:04:51.044931Z",
     "shell.execute_reply": "2024-11-22T05:04:51.044098Z",
     "shell.execute_reply.started": "2024-11-22T05:04:51.037558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before freezing the base model parameters\n",
      "Total parameters: 1,235,818,498\n",
      "Trainable parameters: 1,235,818,498\n",
      "Total Base model parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in qa_model.parameters())\n",
    "base_total_params = sum(p.numel() for p in qa_model.base_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in qa_model.parameters() if p.requires_grad)\n",
    "print(\"Parameters before freezing the base model parameters\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total Base model parameters: {base_total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the Base Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:51.046186Z",
     "iopub.status.busy": "2024-11-22T05:04:51.045948Z",
     "iopub.status.idle": "2024-11-22T05:04:51.059147Z",
     "shell.execute_reply": "2024-11-22T05:04:51.058352Z",
     "shell.execute_reply.started": "2024-11-22T05:04:51.046163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in qa_model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Number of Parameters after Freezing the Base Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:51.060912Z",
     "iopub.status.busy": "2024-11-22T05:04:51.060112Z",
     "iopub.status.idle": "2024-11-22T05:04:51.070926Z",
     "shell.execute_reply": "2024-11-22T05:04:51.070256Z",
     "shell.execute_reply.started": "2024-11-22T05:04:51.060885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after Freezing the Base Model Parameters:\n",
      "Total parameters: 1,235,818,498\n",
      "Trainable parameters: 4,098\n",
      "Total Base model parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in qa_model.parameters())\n",
    "base_total_params = sum(p.numel() for p in qa_model.base_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in qa_model.parameters() if p.requires_grad)\n",
    "print(\"Parameters after Freezing the Base Model Parameters:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total Base model parameters: {base_total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:51.071972Z",
     "iopub.status.busy": "2024-11-22T05:04:51.071749Z",
     "iopub.status.idle": "2024-11-22T05:04:56.119958Z",
     "shell.execute_reply": "2024-11-22T05:04:56.119013Z",
     "shell.execute_reply.started": "2024-11-22T05:04:51.071949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "exact_match_metric = load(\"exact_match\")\n",
    "f1_metric = load(\"f1\")\n",
    "bleu_metric = load(\"bleu\")\n",
    "rouge_metric = load(\"rouge\")\n",
    "meteor_metric = load(\"meteor\")\n",
    "squad_metric = load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:56.121560Z",
     "iopub.status.busy": "2024-11-22T05:04:56.121283Z",
     "iopub.status.idle": "2024-11-22T05:04:56.135149Z",
     "shell.execute_reply": "2024-11-22T05:04:56.134547Z",
     "shell.execute_reply.started": "2024-11-22T05:04:56.121532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, device, dataset, tokenizer, metrics=[\"f1\", \"squad_v2\", \"bleu\", \"exact_match\", \"meteor\", \"rouge\"]):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    predictions = []\n",
    "    references = []\n",
    "    print(\"Evaluating...\")\n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(\n",
    "            example[\"question\"],\n",
    "            example[\"context\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "        start_logits = outputs[\"start_logits\"].cpu().numpy()\n",
    "        end_logits = outputs[\"end_logits\"].cpu().numpy()\n",
    "\n",
    "\n",
    "        start_idx = np.argmax(start_logits)\n",
    "        end_idx = np.argmax(end_logits)\n",
    "\n",
    "        predicted_text = tokenizer.decode(\n",
    "            inputs[\"input_ids\"][0][start_idx:end_idx + 1],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        predictions.append(predicted_text)\n",
    "\n",
    "        # Ground truth answer\n",
    "        references.append(example[\"answers\"])\n",
    "\n",
    "    # Calculate metrics\n",
    "    results = {}\n",
    "    if \"f1\" in metrics:\n",
    "        f1_scores = [\n",
    "            compute_f1(pred, ref) for pred, ref in zip(predictions, references)\n",
    "        ]\n",
    "        results[\"f1\"] = np.mean(f1_scores)\n",
    "\n",
    "    if \"exact_match\" in metrics:\n",
    "        em_scores = [\n",
    "            compute_exact_match(pred, ref) for pred, ref in zip(predictions, references)\n",
    "        ]\n",
    "        results[\"exact_match\"] = np.mean(em_scores)\n",
    "\n",
    "    if \"bleu\" in metrics:\n",
    "        valid_references = [[r] for r in references]\n",
    "        if predictions and valid_references:\n",
    "            try:\n",
    "                bleu_scores = bleu_metric.compute(\n",
    "                    predictions=predictions,\n",
    "                    references=valid_references\n",
    "                )\n",
    "                results[\"bleu\"] = bleu_scores[\"bleu\"]\n",
    "            except ZeroDivisionError as e:\n",
    "                print(\"ZeroDivisionError during BLEU calculation. Returning partial result.\")\n",
    "                results[\"bleu\"] = '0.0 (because ZeroDivisionError during BLEU Calculation)'\n",
    "        else:\n",
    "            print(\"No valid data points for BLEU calculation. Setting BLEU score to 0.\")\n",
    "            results[\"bleu\"] = '0.0 (because no valid data points for BLEU Calculation)'\n",
    "\n",
    "\n",
    "    if \"meteor\" in metrics:\n",
    "        meteor_scores = meteor_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )\n",
    "        results[\"meteor\"] = meteor_scores[\"meteor\"]\n",
    "\n",
    "    if \"rouge\" in metrics:\n",
    "        rouge_scores = rouge_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )\n",
    "        results[\"rouge\"] = rouge_scores\n",
    "    \n",
    "    if \"squad_v2\" in metrics:\n",
    "        # Restructure predictions\n",
    "        formatted_predictions = [\n",
    "            {\n",
    "                \"id\": example[\"id\"],\n",
    "                \"prediction_text\": prediction,\n",
    "                \"no_answer_probability\": 0.0  # Adjust if using no-answer probabilities\n",
    "            }\n",
    "            for example, prediction in zip(dataset, predictions)\n",
    "        ]\n",
    "    \n",
    "        # Restructure references\n",
    "        formatted_references = [\n",
    "            {\n",
    "                \"id\": example[\"id\"],\n",
    "                \"answers\": [{\"text\": example[\"answers\"], \"answer_start\": example[\"start_positions\"]}]\n",
    "            }\n",
    "            for example in dataset\n",
    "        ]\n",
    "    \n",
    "        # Compute SQuAD v2 scores\n",
    "        squad_v2_scores = squad_metric.compute(\n",
    "            predictions=formatted_predictions,\n",
    "            references=formatted_references\n",
    "        )\n",
    "        results[\"squad_v2\"] = squad_v2_scores\n",
    "    for key in metrics:\n",
    "        if(type(results[key]) != dict):\n",
    "            print(key, \":\", results[key])\n",
    "        else:\n",
    "            print(key, \":\")\n",
    "            pprint(results[key])\n",
    "\n",
    "from pprint import pprint\n",
    "def compute_f1(pred, ref):\n",
    "    pred_tokens = pred.split()\n",
    "    ref_tokens = ref.split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_same = len(common)\n",
    "\n",
    "    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "        return int(pred_tokens == ref_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(ref_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_exact_match(pred, ref):\n",
    "    return int(pred.strip() == ref.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:04:56.136527Z",
     "iopub.status.busy": "2024-11-22T05:04:56.136202Z",
     "iopub.status.idle": "2024-11-22T05:07:50.778148Z",
     "shell.execute_reply": "2024-11-22T05:07:50.777381Z",
     "shell.execute_reply.started": "2024-11-22T05:04:56.136476Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 : 0.3144356825189179\n",
      "squad_v2 :\n",
      "{'HasAns_exact': 30.2,\n",
      " 'HasAns_f1': 31.836490539007123,\n",
      " 'HasAns_total': 500,\n",
      " 'best_exact': 30.2,\n",
      " 'best_exact_thresh': 0.0,\n",
      " 'best_f1': 31.836490539007123,\n",
      " 'best_f1_thresh': 0.0,\n",
      " 'exact': 30.2,\n",
      " 'f1': 31.836490539007123,\n",
      " 'total': 500}\n",
      "bleu : 0.004205215763694626\n",
      "exact_match : 0.302\n",
      "meteor : 0.028120382620535067\n",
      "rouge :\n",
      "{'rouge1': 0.017202131138604027,\n",
      " 'rouge2': 0.005994900218554977,\n",
      " 'rougeL': 0.016735073197505806,\n",
      " 'rougeLsum': 0.01678976408627315}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(qa_model, device, test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:07:50.779456Z",
     "iopub.status.busy": "2024-11-22T05:07:50.779199Z",
     "iopub.status.idle": "2024-11-22T05:07:51.834413Z",
     "shell.execute_reply": "2024-11-22T05:07:51.833561Z",
     "shell.execute_reply.started": "2024-11-22T05:07:50.779431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"wandb\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:07:51.835986Z",
     "iopub.status.busy": "2024-11-22T05:07:51.835646Z",
     "iopub.status.idle": "2024-11-22T05:07:51.870289Z",
     "shell.execute_reply": "2024-11-22T05:07:51.869545Z",
     "shell.execute_reply.started": "2024-11-22T05:07:51.835948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=qa_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:07:51.871707Z",
     "iopub.status.busy": "2024-11-22T05:07:51.871368Z",
     "iopub.status.idle": "2024-11-22T06:28:12.916052Z",
     "shell.execute_reply": "2024-11-22T06:28:12.915071Z",
     "shell.execute_reply.started": "2024-11-22T05:07:51.871666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1768dc972254ace915ddcd452f05c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112965733333466, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241122_050951-wbl967m9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface/runs/wbl967m9' target=\"_blank\">llama_telugu</a></strong> to <a href='https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface/runs/wbl967m9' target=\"_blank\">https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface/runs/wbl967m9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 1:17:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.834700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"huggingface\", name=\"llama_telugu\")\n",
    "wandb.watch(qa_model, log=\"all\", log_freq=100)\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T03:24:01.756870Z",
     "iopub.status.busy": "2024-11-22T03:24:01.756480Z",
     "iopub.status.idle": "2024-11-22T03:29:32.369816Z",
     "shell.execute_reply": "2024-11-22T03:29:32.368947Z",
     "shell.execute_reply.started": "2024-11-22T03:24:01.756830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "f1 : 0.06528022822167888\n",
      "squad_v2 :\n",
      "{'HasAns_exact': 5.0,\n",
      " 'HasAns_f1': 7.761453452808612,\n",
      " 'HasAns_total': 500,\n",
      " 'best_exact': 5.0,\n",
      " 'best_exact_thresh': 0.0,\n",
      " 'best_f1': 7.761453452808612,\n",
      " 'best_f1_thresh': 0.0,\n",
      " 'exact': 5.0,\n",
      " 'f1': 7.761453452808612,\n",
      " 'total': 500}\n",
      "bleu : 0.005040420825422111\n",
      "exact_match : 0.042\n",
      "meteor : 0.05902862809853723\n",
      "rouge :\n",
      "{'rouge1': 0.027745307977290832,\n",
      " 'rouge2': 0.013237317729345024,\n",
      " 'rougeL': 0.02687145779982931,\n",
      " 'rougeLsum': 0.027163246918168953}\n"
     ]
    }
   ],
   "source": [
    "qa_model.to(device)\n",
    "evaluate_model(qa_model, device, test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T06:28:12.925312Z",
     "iopub.status.busy": "2024-11-22T06:28:12.925061Z",
     "iopub.status.idle": "2024-11-22T06:28:21.369445Z",
     "shell.execute_reply": "2024-11-22T06:28:21.368542Z",
     "shell.execute_reply.started": "2024-11-22T06:28:12.925281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.307 MB of 0.307 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▃▆▂▅▃▅▂▁▂▂▄▅▁▂▂▂▅▂▁▁▁▂▃▂▃▁▁▃▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▂▃▃▃▃▂▃▃▄▂▃▃▃▂▂▃▂▂▂▂▂▃▁▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/runtime</td><td>120.964</td></tr><tr><td>eval/samples_per_second</td><td>4.133</td></tr><tr><td>eval/steps_per_second</td><td>0.265</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>0.9984</td></tr><tr><td>train/global_step</td><td>312</td></tr><tr><td>train/grad_norm</td><td>357127.15625</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>4.8347</td></tr><tr><td>train_loss</td><td>4.92475</td></tr><tr><td>train_runtime</td><td>4687.1713</td></tr><tr><td>train_samples_per_second</td><td>4.267</td></tr><tr><td>train_steps_per_second</td><td>0.067</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama_telugu</strong> at: <a href='https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface/runs/wbl967m9' target=\"_blank\">https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface/runs/wbl967m9</a><br/> View project at: <a href='https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/patelbhavik06-bp-indian-institute-of-technology-gandhinagar/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241122_050951-wbl967m9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T06:28:21.370808Z",
     "iopub.status.busy": "2024-11-22T06:28:21.370525Z",
     "iopub.status.idle": "2024-11-22T06:28:21.374800Z",
     "shell.execute_reply": "2024-11-22T06:28:21.373968Z",
     "shell.execute_reply.started": "2024-11-22T06:28:21.370783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"./saved_model\",\n",
    "    repo_id=\"bp03/QuestionAnswering_SQUADV2_Llamma_3.2_1B\",\n",
    "    commit_message=\"Added fine-tuned model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
