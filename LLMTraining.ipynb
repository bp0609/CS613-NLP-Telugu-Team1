{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from transformers import (\n",
    "    AutoTokenizer, LlamaConfig, LlamaForCausalLM, TrainingArguments, Trainer, PreTrainedTokenizerFast\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo = \"ai4bharat/sangraha\"\n",
    "sb_folder = \"verified/tel\"\n",
    "local_dir = \"ai4bharat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(df):\n",
    "    return sys.getsizeof(df) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(repo, sb_folder, local_dir, range_):\n",
    "    telugu_char_pattern = re.compile(r'[^\\u0C00-\\u0C7F\\u0000-\\u007F\\s<>\\n0-9.,!?]')\n",
    "    filenames = [f\"data-{i}.parquet\" for i in range(*range_)]\n",
    "    df_dataset = pd.DataFrame()\n",
    "    for filename in filenames:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo,\n",
    "            repo_type=\"dataset\",\n",
    "            subfolder=sb_folder,\n",
    "            filename=filename,\n",
    "            local_dir=local_dir\n",
    "        )\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df = df.rename(columns={\"text\": \"Input\"})\n",
    "        df.drop(columns=['type'], inplace=True)\n",
    "        df.drop(columns=['doc_id'], inplace=True)\n",
    "        df[\"Input\"] = \"<bos> \" + df[\"Input\"] + \" <eos>\"\n",
    "        df[\"Input\"] = df[\"Input\"].str.replace(\"\\n\", \"<newline>\")\n",
    "        df[\"Input\"] = df[\"Input\"].apply(lambda x: telugu_char_pattern.sub(\"\", x))\n",
    "        df_dataset = pd.concat([df_dataset, df], ignore_index=True)\n",
    "    \n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bos&gt; వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;bos&gt; ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?&lt;newline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bos&gt; సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bos&gt; మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!&lt;newl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bos&gt; ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174757</th>\n",
       "      <td>&lt;bos&gt; ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174758</th>\n",
       "      <td>&lt;bos&gt; భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>&lt;bos&gt; నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174760</th>\n",
       "      <td>&lt;bos&gt; తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174761</th>\n",
       "      <td>&lt;bos&gt; 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174762 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Input\n",
       "0       <bos> వెండితెరపై లవర్ బాయ్స్ చాలామంది ఉన్నారు....\n",
       "1       <bos> ఎల్ఐసీ పాలసీని మధ్యలోనే ఆపేశారా?<newline...\n",
       "2       <bos> సరికాని ఆహారం మరియు ఉత్పత్తుల నాణ్యమైన న...\n",
       "3       <bos> మహేష్ సినిమాలో విజయశాంతి పాత్ర ఇదే!<newl...\n",
       "4       <bos> ఢిల్లీ : భారతీయ రైల్వే మరో ఘనతను సాధించి...\n",
       "...                                                   ...\n",
       "174757  <bos> ఎ. వెంక టేశ్వరరావు (విజయవాడ), ఎస్. సత్యన...\n",
       "174758  <bos> భారత ప్రధాని హోదాలో అగ్రరాజ్యం అమెరికా ప...\n",
       "174759  <bos> నవతరం దర్శకుల్లో తనదైన అభిరుచిని చాటుకుం...\n",
       "174760  <bos> తన భార్య అనుష్క డెలివరీ నేపథ్యంలో విరాట్...\n",
       "174761  <bos> 1 యేసు దేవుణు గుడిఃదు బస్తాండ్రె, సుటులం...\n",
       "\n",
       "[174762 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_dataset(repo, sb_folder, local_dir, (13, 14))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_telugu = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"SentencePieceBPETokenizer_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Input\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = df.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    max_position_embeddings=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config=llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in model.named_parameters():\n",
    "  if j.requires_grad and len(j.size()) > 1:\n",
    "    init.xavier_uniform_(j.data)\n",
    "    \n",
    "total_param=0\n",
    "for i,j in model.named_parameters():\n",
    "    total_param += j.numel()\n",
    "print(total_param/(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-telugu\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    perplexity = torch.exp(torch.tensor(eval_pred[1].mean()))\n",
    "    return {\"perplexity\": perplexity.item()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "perplexity_log = []\n",
    "for epoch in range(1, 4):\n",
    "    results = trainer.evaluate()\n",
    "    perplexity = results[\"eval_perplexity\"]\n",
    "    perplexity_log.append((epoch, perplexity))\n",
    "    print(f\"Epoch: {epoch}, Perplexity: {perplexity}\")\n",
    "\n",
    "trainer.save_model(\"./llama-telugu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"తెలుగు భాష అనేది\",\n",
    "    \"ఇది ఒక గొప్ప\",\n",
    "    \"మీ పేరేమిటి?\",\n",
    "    \"నేను ఎలా చేయగలను\",\n",
    "    \"కళలు మన\",\n",
    "    \"పుస్తకం చదివితే\",\n",
    "    \"సాహిత్యం అంటే\",\n",
    "    \"ప్రపంచానికి తెలుగు\",\n",
    "    \"తెలుగులో గొప్ప రచనలు\",\n",
    "    \"భావన నిండిన\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_df = pd.DataFrame(perplexity_log, columns=[\"Epoch\", \"Perplexity\"])\n",
    "print(perplexity_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
